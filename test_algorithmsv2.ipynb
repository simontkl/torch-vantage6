{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the params (tensors)\n",
    "\n",
    "# model = Net()\n",
    "\n",
    "# for parameter in model.parameters():\n",
    "#     print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def RPC_initialize_training(data, gamma, learning_rate, local_dp):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # print(\"\\033[0;{};49m Rank {} is training on {}\".format(device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "                sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "                max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train(data, log_interval, local_dp, epoch, round, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        log_interval: The amount of rounds before logging intermediate loss.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "    \"\"\"\n",
    "    # loading arguments/parameters from first RPC_method\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp) # is this allowed in vantage6? calling one RPC_method in another?\n",
    "     \n",
    "    train_loader = data\n",
    "    \n",
    "    model.train()\n",
    "# for batch_idx, (data, target) in enumarate(train_loader):\n",
    "    for batch_idx, data in enumerate(train_loader, 0): \n",
    "        data, target = data\n",
    "        # Send the data and target to the device (cpu/gpu) the model is at\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Clear gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model on the data\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "#     Adding differential privacy or not\n",
    "    if local_dp == True:  \n",
    "        epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            # print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))\n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# # This function trains the neural network for one epoch\n",
    "# def train(epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # Move the input and target data on the GPU\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # Zero out gradients from previous step\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass of the neural net\n",
    "#         output = model(data)\n",
    "#         # Calculation of the loss function\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         # Backward pass (gradient computation)\n",
    "#         loss.backward()\n",
    "#         # Adjusting the parameters according to the loss function\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def RPC_test(data):\n",
    "    \"\"\"\n",
    "    Tests the model.\n",
    "\n",
    "    Args:\n",
    "        color: The color for the terminal output for this worker.\n",
    "        model: The model to test.\n",
    "        device: The device to test the model on.\n",
    "        test_loader: The local loader for test local. -> no inside function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#     test_loader = torch.load(\"./testing.pt\")\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                              train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(test_loss)\n",
    "\n",
    "    # print('\\033[0;{};49m \\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #         test_loss, correct, len(test_loader.dataset),\n",
    "    #         100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def RPC_get_parameters(data, model, parameters):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    data_size = len(data) // 3 # number of nodes# size of dataset\n",
    "    \n",
    "    weights = []\n",
    "    # Gather the data sizes on the server\n",
    "    tensor_weights = torch.tensor(data_size)\n",
    "    tensor_weights = tensor_weights[1:]\n",
    "    # Convert all tensors back to weights\n",
    "    for tensor in tensor_weights:\n",
    "            weights.append(tensor.item())\n",
    "\n",
    "            \n",
    "    for parameters in model.parameters():\n",
    "        return {\n",
    "            \"params\": parameters,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this might need to be combined with training, so that train \n",
    "returns the parameters or that it at least calls the results of training function\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def average_parameters(data, model):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters() # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for parameters in model.parameters():\n",
    "            average = sum(x * y for x, y in zip(parameters[i], weights)) / sum(weights)\n",
    "            parameters.data = average\n",
    "            i = i + 1\n",
    "        return {\n",
    "            \"params_averaged\": model\n",
    "        }\n",
    "    \n",
    "\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#     # The first entry of the provided parameters when using dist.gather\n",
    "#     # method also contains the value from the server, remove that one\n",
    "#     minus_server = parameters[i][1:]\n",
    "#     # Calculate the average by summing and dividing by the number of\n",
    "#     # workers\n",
    "#     s = sum(minus_server)\n",
    "#     average = s/len(minus_server)\n",
    "#     # Change the parameter of the global model to the average\n",
    "#     param.data = average\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, local_dp, model, device, optimizer, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "    model = RPC_average_parameters()\n",
    "    \n",
    "    for epoch in range(1, epoch + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train(data, local_dp, model, device, optimizer, epoch, delta=1e-5)\n",
    "        # Test the model on the workers\n",
    "        RPC_test(data, model, device)\n",
    "\n",
    "    gather_params = model.get_parameters() # or model.parameters()\n",
    "\n",
    "    RPC_train(model.RPC_average_parameters_weighted(gather_params))\n",
    "\n",
    "    return model, parameters\n",
    "\n",
    "\n",
    "    ## OR \n",
    "\n",
    "#     parameters = RPC_average_parameters_weighted(data, model, parameters, weights) # then uses those parameters for training\n",
    "\n",
    "\n",
    "\n",
    "        # # Gather the parameters after the training round on the server\n",
    "        #     gather_params = coor.gather_parameters(rank, model, group_size + 1, subgroup)\n",
    "        #\n",
    "        #     # If the server\n",
    "        #     if rank == 0:\n",
    "        #         # Calculate the average of the parameters and adjust global model\n",
    "        #         coor.average_parameters_weighted(model, gather_params, weights)\n",
    "        #\n",
    "        #     # Send the new model parameters to the workers\n",
    "        #     coor.broadcast_parameters(model, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTODO, the RPC_initialize_training method is \\ncalled in the RPC_train method, \\nand yet it doesn't know where to get model, device, optimizer from. \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "These are the parameters needed for the function\n",
    "Data loading and transforming (this will be done beforehand \n",
    "and then stored in './local/training.pt' and './testing.pt')\n",
    "\"\"\"\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "gamma=0.7\n",
    "\n",
    "data = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True,\n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "local_dp = True\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:518: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313097\n",
      "Train Epoch: 1 [50/60000 (0%)]\tLoss: 2.338604\n",
      "Train Epoch: 1 [100/60000 (0%)]\tLoss: 2.277131\n",
      "Train Epoch: 1 [150/60000 (0%)]\tLoss: 2.299821\n",
      "Train Epoch: 1 [200/60000 (0%)]\tLoss: 2.292620\n",
      "Train Epoch: 1 [250/60000 (0%)]\tLoss: 2.256905\n",
      "Train Epoch: 1 [300/60000 (0%)]\tLoss: 2.276335\n",
      "Train Epoch: 1 [350/60000 (1%)]\tLoss: 2.327846\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 2.372798\n",
      "Train Epoch: 1 [450/60000 (1%)]\tLoss: 2.281914\n",
      "Train Epoch: 1 [500/60000 (1%)]\tLoss: 2.356257\n",
      "Train Epoch: 1 [550/60000 (1%)]\tLoss: 2.279763\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 2.246746\n",
      "Train Epoch: 1 [650/60000 (1%)]\tLoss: 2.389159\n",
      "Train Epoch: 1 [700/60000 (1%)]\tLoss: 2.345253\n",
      "Train Epoch: 1 [750/60000 (1%)]\tLoss: 2.185812\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 2.302603\n",
      "Train Epoch: 1 [850/60000 (1%)]\tLoss: 2.205883\n",
      "Train Epoch: 1 [900/60000 (2%)]\tLoss: 2.197359\n",
      "Train Epoch: 1 [950/60000 (2%)]\tLoss: 2.287045\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.281614\n",
      "Train Epoch: 1 [1050/60000 (2%)]\tLoss: 2.216887\n",
      "Train Epoch: 1 [1100/60000 (2%)]\tLoss: 2.400283\n",
      "Train Epoch: 1 [1150/60000 (2%)]\tLoss: 2.323077\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 2.199936\n",
      "Train Epoch: 1 [1250/60000 (2%)]\tLoss: 2.338012\n",
      "Train Epoch: 1 [1300/60000 (2%)]\tLoss: 2.123765\n",
      "Train Epoch: 1 [1350/60000 (2%)]\tLoss: 2.410690\n",
      "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 2.278992\n",
      "Train Epoch: 1 [1450/60000 (2%)]\tLoss: 2.300720\n",
      "Train Epoch: 1 [1500/60000 (2%)]\tLoss: 2.362377\n",
      "Train Epoch: 1 [1550/60000 (3%)]\tLoss: 2.299396\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 2.314461\n",
      "Train Epoch: 1 [1650/60000 (3%)]\tLoss: 2.201319\n",
      "Train Epoch: 1 [1700/60000 (3%)]\tLoss: 2.296305\n",
      "Train Epoch: 1 [1750/60000 (3%)]\tLoss: 2.353934\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 2.235421\n",
      "Train Epoch: 1 [1850/60000 (3%)]\tLoss: 2.134496\n",
      "Train Epoch: 1 [1900/60000 (3%)]\tLoss: 2.266987\n",
      "Train Epoch: 1 [1950/60000 (3%)]\tLoss: 2.262616\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.139966\n",
      "Train Epoch: 1 [2050/60000 (3%)]\tLoss: 2.296971\n",
      "Train Epoch: 1 [2100/60000 (4%)]\tLoss: 2.470164\n",
      "Train Epoch: 1 [2150/60000 (4%)]\tLoss: 2.403211\n",
      "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 2.307654\n",
      "Train Epoch: 1 [2250/60000 (4%)]\tLoss: 2.448816\n",
      "Train Epoch: 1 [2300/60000 (4%)]\tLoss: 2.236073\n",
      "Train Epoch: 1 [2350/60000 (4%)]\tLoss: 2.259215\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 1.919226\n",
      "Train Epoch: 1 [2450/60000 (4%)]\tLoss: 2.215497\n",
      "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 2.155372\n",
      "Train Epoch: 1 [2550/60000 (4%)]\tLoss: 2.125364\n",
      "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 2.281129\n",
      "Train Epoch: 1 [2650/60000 (4%)]\tLoss: 2.416352\n",
      "Train Epoch: 1 [2700/60000 (4%)]\tLoss: 2.443266\n",
      "Train Epoch: 1 [2750/60000 (5%)]\tLoss: 2.470730\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 2.447955\n",
      "Train Epoch: 1 [2850/60000 (5%)]\tLoss: 2.430635\n",
      "Train Epoch: 1 [2900/60000 (5%)]\tLoss: 2.341272\n",
      "Train Epoch: 1 [2950/60000 (5%)]\tLoss: 2.016865\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.298137\n",
      "Train Epoch: 1 [3050/60000 (5%)]\tLoss: 2.139151\n",
      "Train Epoch: 1 [3100/60000 (5%)]\tLoss: 2.001222\n",
      "Train Epoch: 1 [3150/60000 (5%)]\tLoss: 2.385499\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.122805\n",
      "Train Epoch: 1 [3250/60000 (5%)]\tLoss: 2.054211\n",
      "Train Epoch: 1 [3300/60000 (6%)]\tLoss: 1.846325\n",
      "Train Epoch: 1 [3350/60000 (6%)]\tLoss: 2.174189\n",
      "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 2.698337\n",
      "Train Epoch: 1 [3450/60000 (6%)]\tLoss: 2.236253\n",
      "Train Epoch: 1 [3500/60000 (6%)]\tLoss: 2.286360\n",
      "Train Epoch: 1 [3550/60000 (6%)]\tLoss: 1.922731\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 2.407802\n",
      "Train Epoch: 1 [3650/60000 (6%)]\tLoss: 2.005260\n",
      "Train Epoch: 1 [3700/60000 (6%)]\tLoss: 2.242796\n",
      "Train Epoch: 1 [3750/60000 (6%)]\tLoss: 2.193736\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 1.996151\n",
      "Train Epoch: 1 [3850/60000 (6%)]\tLoss: 2.231858\n",
      "Train Epoch: 1 [3900/60000 (6%)]\tLoss: 1.855508\n",
      "Train Epoch: 1 [3950/60000 (7%)]\tLoss: 1.764932\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.351017\n",
      "Train Epoch: 1 [4050/60000 (7%)]\tLoss: 2.276380\n",
      "Train Epoch: 1 [4100/60000 (7%)]\tLoss: 2.038070\n",
      "Train Epoch: 1 [4150/60000 (7%)]\tLoss: 2.210661\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 2.076357\n",
      "Train Epoch: 1 [4250/60000 (7%)]\tLoss: 2.079155\n",
      "Train Epoch: 1 [4300/60000 (7%)]\tLoss: 2.086456\n",
      "Train Epoch: 1 [4350/60000 (7%)]\tLoss: 1.964518\n",
      "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 2.089401\n",
      "Train Epoch: 1 [4450/60000 (7%)]\tLoss: 1.948421\n",
      "Train Epoch: 1 [4500/60000 (8%)]\tLoss: 2.163627\n",
      "Train Epoch: 1 [4550/60000 (8%)]\tLoss: 1.826736\n",
      "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 2.108795\n",
      "Train Epoch: 1 [4650/60000 (8%)]\tLoss: 2.039340\n",
      "Train Epoch: 1 [4700/60000 (8%)]\tLoss: 2.646444\n",
      "Train Epoch: 1 [4750/60000 (8%)]\tLoss: 2.138525\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 2.219581\n",
      "Train Epoch: 1 [4850/60000 (8%)]\tLoss: 1.969191\n",
      "Train Epoch: 1 [4900/60000 (8%)]\tLoss: 1.897004\n",
      "Train Epoch: 1 [4950/60000 (8%)]\tLoss: 2.405215\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 1.764804\n",
      "Train Epoch: 1 [5050/60000 (8%)]\tLoss: 2.225683\n",
      "Train Epoch: 1 [5100/60000 (8%)]\tLoss: 2.335071\n",
      "Train Epoch: 1 [5150/60000 (9%)]\tLoss: 2.238412\n",
      "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 2.206374\n",
      "Train Epoch: 1 [5250/60000 (9%)]\tLoss: 1.913268\n",
      "Train Epoch: 1 [5300/60000 (9%)]\tLoss: 2.507144\n",
      "Train Epoch: 1 [5350/60000 (9%)]\tLoss: 2.271866\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 2.074369\n",
      "Train Epoch: 1 [5450/60000 (9%)]\tLoss: 2.346390\n",
      "Train Epoch: 1 [5500/60000 (9%)]\tLoss: 2.058200\n",
      "Train Epoch: 1 [5550/60000 (9%)]\tLoss: 1.834706\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 2.242310\n",
      "Train Epoch: 1 [5650/60000 (9%)]\tLoss: 2.252563\n",
      "Train Epoch: 1 [5700/60000 (10%)]\tLoss: 2.155341\n",
      "Train Epoch: 1 [5750/60000 (10%)]\tLoss: 2.379258\n",
      "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 2.118857\n",
      "Train Epoch: 1 [5850/60000 (10%)]\tLoss: 2.213362\n",
      "Train Epoch: 1 [5900/60000 (10%)]\tLoss: 1.667224\n",
      "Train Epoch: 1 [5950/60000 (10%)]\tLoss: 2.096028\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.223009\n",
      "Train Epoch: 1 [6050/60000 (10%)]\tLoss: 2.464029\n",
      "Train Epoch: 1 [6100/60000 (10%)]\tLoss: 1.961027\n",
      "Train Epoch: 1 [6150/60000 (10%)]\tLoss: 2.124690\n",
      "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 2.120842\n",
      "Train Epoch: 1 [6250/60000 (10%)]\tLoss: 2.063372\n",
      "Train Epoch: 1 [6300/60000 (10%)]\tLoss: 1.628852\n",
      "Train Epoch: 1 [6350/60000 (11%)]\tLoss: 1.497431\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.089219\n",
      "Train Epoch: 1 [6450/60000 (11%)]\tLoss: 2.071619\n",
      "Train Epoch: 1 [6500/60000 (11%)]\tLoss: 2.005233\n",
      "Train Epoch: 1 [6550/60000 (11%)]\tLoss: 1.597031\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 2.563761\n",
      "Train Epoch: 1 [6650/60000 (11%)]\tLoss: 2.553051\n",
      "Train Epoch: 1 [6700/60000 (11%)]\tLoss: 1.473946\n",
      "Train Epoch: 1 [6750/60000 (11%)]\tLoss: 2.532992\n",
      "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 1.820718\n",
      "Train Epoch: 1 [6850/60000 (11%)]\tLoss: 1.990359\n",
      "Train Epoch: 1 [6900/60000 (12%)]\tLoss: 2.397919\n",
      "Train Epoch: 1 [6950/60000 (12%)]\tLoss: 1.942052\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.261345\n",
      "Train Epoch: 1 [7050/60000 (12%)]\tLoss: 2.158335\n",
      "Train Epoch: 1 [7100/60000 (12%)]\tLoss: 2.400031\n",
      "Train Epoch: 1 [7150/60000 (12%)]\tLoss: 2.071277\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 2.235700\n",
      "Train Epoch: 1 [7250/60000 (12%)]\tLoss: 1.865101\n",
      "Train Epoch: 1 [7300/60000 (12%)]\tLoss: 2.092355\n",
      "Train Epoch: 1 [7350/60000 (12%)]\tLoss: 2.039027\n",
      "Train Epoch: 1 [7400/60000 (12%)]\tLoss: 1.957212\n",
      "Train Epoch: 1 [7450/60000 (12%)]\tLoss: 1.909876\n",
      "Train Epoch: 1 [7500/60000 (12%)]\tLoss: 1.674314\n",
      "Train Epoch: 1 [7550/60000 (13%)]\tLoss: 2.204896\n",
      "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 1.872671\n",
      "Train Epoch: 1 [7650/60000 (13%)]\tLoss: 1.664920\n",
      "Train Epoch: 1 [7700/60000 (13%)]\tLoss: 2.410659\n",
      "Train Epoch: 1 [7750/60000 (13%)]\tLoss: 2.671709\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 2.049358\n",
      "Train Epoch: 1 [7850/60000 (13%)]\tLoss: 2.089913\n",
      "Train Epoch: 1 [7900/60000 (13%)]\tLoss: 2.090447\n",
      "Train Epoch: 1 [7950/60000 (13%)]\tLoss: 2.596529\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.879488\n",
      "Train Epoch: 1 [8050/60000 (13%)]\tLoss: 1.562477\n",
      "Train Epoch: 1 [8100/60000 (14%)]\tLoss: 2.081933\n",
      "Train Epoch: 1 [8150/60000 (14%)]\tLoss: 1.768658\n",
      "Train Epoch: 1 [8200/60000 (14%)]\tLoss: 1.969295\n",
      "Train Epoch: 1 [8250/60000 (14%)]\tLoss: 2.415565\n",
      "Train Epoch: 1 [8300/60000 (14%)]\tLoss: 1.901284\n",
      "Train Epoch: 1 [8350/60000 (14%)]\tLoss: 2.828379\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 1.786345\n",
      "Train Epoch: 1 [8450/60000 (14%)]\tLoss: 2.103007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [8500/60000 (14%)]\tLoss: 2.248651\n",
      "Train Epoch: 1 [8550/60000 (14%)]\tLoss: 2.456940\n",
      "Train Epoch: 1 [8600/60000 (14%)]\tLoss: 2.071569\n",
      "Train Epoch: 1 [8650/60000 (14%)]\tLoss: 1.958503\n",
      "Train Epoch: 1 [8700/60000 (14%)]\tLoss: 2.096431\n",
      "Train Epoch: 1 [8750/60000 (15%)]\tLoss: 2.321998\n",
      "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 2.328505\n",
      "Train Epoch: 1 [8850/60000 (15%)]\tLoss: 2.347884\n",
      "Train Epoch: 1 [8900/60000 (15%)]\tLoss: 1.588742\n",
      "Train Epoch: 1 [8950/60000 (15%)]\tLoss: 1.949710\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 2.130940\n",
      "Train Epoch: 1 [9050/60000 (15%)]\tLoss: 2.076988\n",
      "Train Epoch: 1 [9100/60000 (15%)]\tLoss: 2.505519\n",
      "Train Epoch: 1 [9150/60000 (15%)]\tLoss: 1.711868\n",
      "Train Epoch: 1 [9200/60000 (15%)]\tLoss: 2.649544\n",
      "Train Epoch: 1 [9250/60000 (15%)]\tLoss: 2.235105\n",
      "Train Epoch: 1 [9300/60000 (16%)]\tLoss: 2.983631\n",
      "Train Epoch: 1 [9350/60000 (16%)]\tLoss: 2.443490\n",
      "Train Epoch: 1 [9400/60000 (16%)]\tLoss: 2.536649\n",
      "Train Epoch: 1 [9450/60000 (16%)]\tLoss: 2.554959\n",
      "Train Epoch: 1 [9500/60000 (16%)]\tLoss: 2.034064\n",
      "Train Epoch: 1 [9550/60000 (16%)]\tLoss: 2.332165\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.013086\n",
      "Train Epoch: 1 [9650/60000 (16%)]\tLoss: 2.321369\n",
      "Train Epoch: 1 [9700/60000 (16%)]\tLoss: 2.654302\n",
      "Train Epoch: 1 [9750/60000 (16%)]\tLoss: 2.039563\n",
      "Train Epoch: 1 [9800/60000 (16%)]\tLoss: 1.869598\n",
      "Train Epoch: 1 [9850/60000 (16%)]\tLoss: 2.312859\n",
      "Train Epoch: 1 [9900/60000 (16%)]\tLoss: 2.366481\n",
      "Train Epoch: 1 [9950/60000 (17%)]\tLoss: 2.125800\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.454083\n",
      "Train Epoch: 1 [10050/60000 (17%)]\tLoss: 3.021964\n",
      "Train Epoch: 1 [10100/60000 (17%)]\tLoss: 2.869664\n",
      "Train Epoch: 1 [10150/60000 (17%)]\tLoss: 1.574821\n",
      "Train Epoch: 1 [10200/60000 (17%)]\tLoss: 2.023672\n",
      "Train Epoch: 1 [10250/60000 (17%)]\tLoss: 1.597275\n",
      "Train Epoch: 1 [10300/60000 (17%)]\tLoss: 1.586688\n",
      "Train Epoch: 1 [10350/60000 (17%)]\tLoss: 1.924754\n",
      "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 3.066037\n",
      "Train Epoch: 1 [10450/60000 (17%)]\tLoss: 1.800006\n",
      "Train Epoch: 1 [10500/60000 (18%)]\tLoss: 2.116042\n",
      "Train Epoch: 1 [10550/60000 (18%)]\tLoss: 2.603780\n",
      "Train Epoch: 1 [10600/60000 (18%)]\tLoss: 2.436964\n",
      "Train Epoch: 1 [10650/60000 (18%)]\tLoss: 2.582261\n",
      "Train Epoch: 1 [10700/60000 (18%)]\tLoss: 2.677217\n",
      "Train Epoch: 1 [10750/60000 (18%)]\tLoss: 2.156619\n",
      "Train Epoch: 1 [10800/60000 (18%)]\tLoss: 1.910672\n",
      "Train Epoch: 1 [10850/60000 (18%)]\tLoss: 2.473547\n",
      "Train Epoch: 1 [10900/60000 (18%)]\tLoss: 1.856631\n",
      "Train Epoch: 1 [10950/60000 (18%)]\tLoss: 2.766474\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 2.492661\n",
      "Train Epoch: 1 [11050/60000 (18%)]\tLoss: 1.798382\n",
      "Train Epoch: 1 [11100/60000 (18%)]\tLoss: 2.524935\n",
      "Train Epoch: 1 [11150/60000 (19%)]\tLoss: 2.153845\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 1.727288\n",
      "Train Epoch: 1 [11250/60000 (19%)]\tLoss: 1.579100\n",
      "Train Epoch: 1 [11300/60000 (19%)]\tLoss: 3.061354\n",
      "Train Epoch: 1 [11350/60000 (19%)]\tLoss: 1.537040\n",
      "Train Epoch: 1 [11400/60000 (19%)]\tLoss: 2.665307\n",
      "Train Epoch: 1 [11450/60000 (19%)]\tLoss: 2.918139\n",
      "Train Epoch: 1 [11500/60000 (19%)]\tLoss: 1.328001\n",
      "Train Epoch: 1 [11550/60000 (19%)]\tLoss: 2.350220\n",
      "Train Epoch: 1 [11600/60000 (19%)]\tLoss: 1.910156\n",
      "Train Epoch: 1 [11650/60000 (19%)]\tLoss: 1.826861\n",
      "Train Epoch: 1 [11700/60000 (20%)]\tLoss: 2.632097\n",
      "Train Epoch: 1 [11750/60000 (20%)]\tLoss: 2.358521\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tLoss: 1.725042\n",
      "Train Epoch: 1 [11850/60000 (20%)]\tLoss: 2.664956\n",
      "Train Epoch: 1 [11900/60000 (20%)]\tLoss: 2.097854\n",
      "Train Epoch: 1 [11950/60000 (20%)]\tLoss: 2.259754\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 2.201731\n",
      "Train Epoch: 1 [12050/60000 (20%)]\tLoss: 1.939779\n",
      "Train Epoch: 1 [12100/60000 (20%)]\tLoss: 1.732136\n",
      "Train Epoch: 1 [12150/60000 (20%)]\tLoss: 1.654985\n",
      "Train Epoch: 1 [12200/60000 (20%)]\tLoss: 2.158988\n",
      "Train Epoch: 1 [12250/60000 (20%)]\tLoss: 2.224838\n",
      "Train Epoch: 1 [12300/60000 (20%)]\tLoss: 2.770108\n",
      "Train Epoch: 1 [12350/60000 (21%)]\tLoss: 1.292288\n",
      "Train Epoch: 1 [12400/60000 (21%)]\tLoss: 2.330332\n",
      "Train Epoch: 1 [12450/60000 (21%)]\tLoss: 1.602536\n",
      "Train Epoch: 1 [12500/60000 (21%)]\tLoss: 2.884931\n",
      "Train Epoch: 1 [12550/60000 (21%)]\tLoss: 1.711745\n",
      "Train Epoch: 1 [12600/60000 (21%)]\tLoss: 1.702829\n",
      "Train Epoch: 1 [12650/60000 (21%)]\tLoss: 2.135332\n",
      "Train Epoch: 1 [12700/60000 (21%)]\tLoss: 2.190712\n",
      "Train Epoch: 1 [12750/60000 (21%)]\tLoss: 1.625146\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.785328\n",
      "Train Epoch: 1 [12850/60000 (21%)]\tLoss: 2.448761\n",
      "Train Epoch: 1 [12900/60000 (22%)]\tLoss: 1.779715\n",
      "Train Epoch: 1 [12950/60000 (22%)]\tLoss: 1.877416\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 1.427129\n",
      "Train Epoch: 1 [13050/60000 (22%)]\tLoss: 1.813611\n",
      "Train Epoch: 1 [13100/60000 (22%)]\tLoss: 2.579524\n",
      "Train Epoch: 1 [13150/60000 (22%)]\tLoss: 1.740680\n",
      "Train Epoch: 1 [13200/60000 (22%)]\tLoss: 1.694289\n",
      "Train Epoch: 1 [13250/60000 (22%)]\tLoss: 2.492406\n",
      "Train Epoch: 1 [13300/60000 (22%)]\tLoss: 2.422485\n",
      "Train Epoch: 1 [13350/60000 (22%)]\tLoss: 3.105165\n",
      "Train Epoch: 1 [13400/60000 (22%)]\tLoss: 3.248439\n",
      "Train Epoch: 1 [13450/60000 (22%)]\tLoss: 1.980749\n",
      "Train Epoch: 1 [13500/60000 (22%)]\tLoss: 2.047822\n",
      "Train Epoch: 1 [13550/60000 (23%)]\tLoss: 1.876756\n",
      "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 2.325686\n",
      "Train Epoch: 1 [13650/60000 (23%)]\tLoss: 1.609531\n",
      "Train Epoch: 1 [13700/60000 (23%)]\tLoss: 1.672932\n",
      "Train Epoch: 1 [13750/60000 (23%)]\tLoss: 2.446815\n",
      "Train Epoch: 1 [13800/60000 (23%)]\tLoss: 1.666357\n",
      "Train Epoch: 1 [13850/60000 (23%)]\tLoss: 1.471312\n",
      "Train Epoch: 1 [13900/60000 (23%)]\tLoss: 1.754791\n",
      "Train Epoch: 1 [13950/60000 (23%)]\tLoss: 2.603423\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.831662\n",
      "Train Epoch: 1 [14050/60000 (23%)]\tLoss: 2.327077\n",
      "Train Epoch: 1 [14100/60000 (24%)]\tLoss: 1.993737\n",
      "Train Epoch: 1 [14150/60000 (24%)]\tLoss: 2.027079\n",
      "Train Epoch: 1 [14200/60000 (24%)]\tLoss: 2.351825\n",
      "Train Epoch: 1 [14250/60000 (24%)]\tLoss: 1.951800\n",
      "Train Epoch: 1 [14300/60000 (24%)]\tLoss: 1.917866\n",
      "Train Epoch: 1 [14350/60000 (24%)]\tLoss: 1.966753\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 2.018965\n",
      "Train Epoch: 1 [14450/60000 (24%)]\tLoss: 1.484829\n",
      "Train Epoch: 1 [14500/60000 (24%)]\tLoss: 2.791851\n",
      "Train Epoch: 1 [14550/60000 (24%)]\tLoss: 1.623958\n",
      "Train Epoch: 1 [14600/60000 (24%)]\tLoss: 2.892811\n",
      "Train Epoch: 1 [14650/60000 (24%)]\tLoss: 1.481114\n",
      "Train Epoch: 1 [14700/60000 (24%)]\tLoss: 1.482387\n",
      "Train Epoch: 1 [14750/60000 (25%)]\tLoss: 2.134029\n",
      "Train Epoch: 1 [14800/60000 (25%)]\tLoss: 1.824311\n",
      "Train Epoch: 1 [14850/60000 (25%)]\tLoss: 2.028278\n",
      "Train Epoch: 1 [14900/60000 (25%)]\tLoss: 1.725599\n",
      "Train Epoch: 1 [14950/60000 (25%)]\tLoss: 1.850142\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 1.956241\n",
      "Train Epoch: 1 [15050/60000 (25%)]\tLoss: 2.177506\n",
      "Train Epoch: 1 [15100/60000 (25%)]\tLoss: 1.597477\n",
      "Train Epoch: 1 [15150/60000 (25%)]\tLoss: 2.346530\n",
      "Train Epoch: 1 [15200/60000 (25%)]\tLoss: 2.245649\n",
      "Train Epoch: 1 [15250/60000 (25%)]\tLoss: 2.098611\n",
      "Train Epoch: 1 [15300/60000 (26%)]\tLoss: 3.578886\n",
      "Train Epoch: 1 [15350/60000 (26%)]\tLoss: 2.640779\n",
      "Train Epoch: 1 [15400/60000 (26%)]\tLoss: 1.505729\n",
      "Train Epoch: 1 [15450/60000 (26%)]\tLoss: 3.698750\n",
      "Train Epoch: 1 [15500/60000 (26%)]\tLoss: 2.006846\n",
      "Train Epoch: 1 [15550/60000 (26%)]\tLoss: 2.925383\n",
      "Train Epoch: 1 [15600/60000 (26%)]\tLoss: 2.086447\n",
      "Train Epoch: 1 [15650/60000 (26%)]\tLoss: 1.889387\n",
      "Train Epoch: 1 [15700/60000 (26%)]\tLoss: 1.774075\n",
      "Train Epoch: 1 [15750/60000 (26%)]\tLoss: 2.142908\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tLoss: 3.012680\n",
      "Train Epoch: 1 [15850/60000 (26%)]\tLoss: 2.693805\n",
      "Train Epoch: 1 [15900/60000 (26%)]\tLoss: 1.499739\n",
      "Train Epoch: 1 [15950/60000 (27%)]\tLoss: 2.833524\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.242231\n",
      "Train Epoch: 1 [16050/60000 (27%)]\tLoss: 2.327896\n",
      "Train Epoch: 1 [16100/60000 (27%)]\tLoss: 2.643249\n",
      "Train Epoch: 1 [16150/60000 (27%)]\tLoss: 2.620317\n",
      "Train Epoch: 1 [16200/60000 (27%)]\tLoss: 2.011496\n",
      "Train Epoch: 1 [16250/60000 (27%)]\tLoss: 2.082294\n",
      "Train Epoch: 1 [16300/60000 (27%)]\tLoss: 1.487486\n",
      "Train Epoch: 1 [16350/60000 (27%)]\tLoss: 2.555228\n",
      "Train Epoch: 1 [16400/60000 (27%)]\tLoss: 1.557437\n",
      "Train Epoch: 1 [16450/60000 (27%)]\tLoss: 2.273998\n",
      "Train Epoch: 1 [16500/60000 (28%)]\tLoss: 3.468024\n",
      "Train Epoch: 1 [16550/60000 (28%)]\tLoss: 2.864946\n",
      "Train Epoch: 1 [16600/60000 (28%)]\tLoss: 3.099425\n",
      "Train Epoch: 1 [16650/60000 (28%)]\tLoss: 2.610548\n",
      "Train Epoch: 1 [16700/60000 (28%)]\tLoss: 2.289710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16750/60000 (28%)]\tLoss: 1.945506\n",
      "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 3.426509\n",
      "Train Epoch: 1 [16850/60000 (28%)]\tLoss: 2.623998\n",
      "Train Epoch: 1 [16900/60000 (28%)]\tLoss: 1.900130\n",
      "Train Epoch: 1 [16950/60000 (28%)]\tLoss: 2.560344\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 2.785554\n",
      "Train Epoch: 1 [17050/60000 (28%)]\tLoss: 1.502676\n",
      "Train Epoch: 1 [17100/60000 (28%)]\tLoss: 2.528768\n",
      "Train Epoch: 1 [17150/60000 (29%)]\tLoss: 1.863443\n",
      "Train Epoch: 1 [17200/60000 (29%)]\tLoss: 2.589135\n",
      "Train Epoch: 1 [17250/60000 (29%)]\tLoss: 3.618897\n",
      "Train Epoch: 1 [17300/60000 (29%)]\tLoss: 2.992162\n",
      "Train Epoch: 1 [17350/60000 (29%)]\tLoss: 1.723577\n",
      "Train Epoch: 1 [17400/60000 (29%)]\tLoss: 1.891919\n",
      "Train Epoch: 1 [17450/60000 (29%)]\tLoss: 2.462422\n",
      "Train Epoch: 1 [17500/60000 (29%)]\tLoss: 3.132294\n",
      "Train Epoch: 1 [17550/60000 (29%)]\tLoss: 2.676400\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 1.560700\n",
      "Train Epoch: 1 [17650/60000 (29%)]\tLoss: 2.417823\n",
      "Train Epoch: 1 [17700/60000 (30%)]\tLoss: 2.110831\n",
      "Train Epoch: 1 [17750/60000 (30%)]\tLoss: 2.125723\n",
      "Train Epoch: 1 [17800/60000 (30%)]\tLoss: 2.957495\n",
      "Train Epoch: 1 [17850/60000 (30%)]\tLoss: 1.678815\n",
      "Train Epoch: 1 [17900/60000 (30%)]\tLoss: 3.343485\n",
      "Train Epoch: 1 [17950/60000 (30%)]\tLoss: 2.367098\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 2.568736\n",
      "Train Epoch: 1 [18050/60000 (30%)]\tLoss: 2.526086\n",
      "Train Epoch: 1 [18100/60000 (30%)]\tLoss: 2.696273\n",
      "Train Epoch: 1 [18150/60000 (30%)]\tLoss: 3.072208\n",
      "Train Epoch: 1 [18200/60000 (30%)]\tLoss: 2.262909\n",
      "Train Epoch: 1 [18250/60000 (30%)]\tLoss: 1.745794\n",
      "Train Epoch: 1 [18300/60000 (30%)]\tLoss: 2.590625\n",
      "Train Epoch: 1 [18350/60000 (31%)]\tLoss: 2.727519\n",
      "Train Epoch: 1 [18400/60000 (31%)]\tLoss: 3.465834\n",
      "Train Epoch: 1 [18450/60000 (31%)]\tLoss: 2.011158\n",
      "Train Epoch: 1 [18500/60000 (31%)]\tLoss: 2.674626\n",
      "Train Epoch: 1 [18550/60000 (31%)]\tLoss: 1.399353\n",
      "Train Epoch: 1 [18600/60000 (31%)]\tLoss: 3.597593\n",
      "Train Epoch: 1 [18650/60000 (31%)]\tLoss: 1.314924\n",
      "Train Epoch: 1 [18700/60000 (31%)]\tLoss: 2.049381\n",
      "Train Epoch: 1 [18750/60000 (31%)]\tLoss: 2.033553\n",
      "Train Epoch: 1 [18800/60000 (31%)]\tLoss: 2.581664\n",
      "Train Epoch: 1 [18850/60000 (31%)]\tLoss: 1.672518\n",
      "Train Epoch: 1 [18900/60000 (32%)]\tLoss: 2.286761\n",
      "Train Epoch: 1 [18950/60000 (32%)]\tLoss: 2.032877\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 1.812380\n",
      "Train Epoch: 1 [19050/60000 (32%)]\tLoss: 2.161680\n",
      "Train Epoch: 1 [19100/60000 (32%)]\tLoss: 2.063780\n",
      "Train Epoch: 1 [19150/60000 (32%)]\tLoss: 2.551267\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.750394\n",
      "Train Epoch: 1 [19250/60000 (32%)]\tLoss: 3.030997\n",
      "Train Epoch: 1 [19300/60000 (32%)]\tLoss: 2.236556\n",
      "Train Epoch: 1 [19350/60000 (32%)]\tLoss: 2.381893\n",
      "Train Epoch: 1 [19400/60000 (32%)]\tLoss: 1.703598\n",
      "Train Epoch: 1 [19450/60000 (32%)]\tLoss: 2.413766\n",
      "Train Epoch: 1 [19500/60000 (32%)]\tLoss: 2.514459\n",
      "Train Epoch: 1 [19550/60000 (33%)]\tLoss: 2.469526\n",
      "Train Epoch: 1 [19600/60000 (33%)]\tLoss: 2.313066\n",
      "Train Epoch: 1 [19650/60000 (33%)]\tLoss: 2.096134\n",
      "Train Epoch: 1 [19700/60000 (33%)]\tLoss: 2.031935\n",
      "Train Epoch: 1 [19750/60000 (33%)]\tLoss: 2.284499\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tLoss: 3.555762\n",
      "Train Epoch: 1 [19850/60000 (33%)]\tLoss: 2.526238\n",
      "Train Epoch: 1 [19900/60000 (33%)]\tLoss: 2.782966\n",
      "Train Epoch: 1 [19950/60000 (33%)]\tLoss: 2.592036\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.841362\n",
      "Train Epoch: 1 [20050/60000 (33%)]\tLoss: 1.755161\n",
      "Train Epoch: 1 [20100/60000 (34%)]\tLoss: 2.585157\n",
      "Train Epoch: 1 [20150/60000 (34%)]\tLoss: 2.694501\n",
      "Train Epoch: 1 [20200/60000 (34%)]\tLoss: 3.127475\n",
      "Train Epoch: 1 [20250/60000 (34%)]\tLoss: 2.557250\n",
      "Train Epoch: 1 [20300/60000 (34%)]\tLoss: 2.617252\n",
      "Train Epoch: 1 [20350/60000 (34%)]\tLoss: 1.826709\n",
      "Train Epoch: 1 [20400/60000 (34%)]\tLoss: 2.210073\n",
      "Train Epoch: 1 [20450/60000 (34%)]\tLoss: 2.555429\n",
      "Train Epoch: 1 [20500/60000 (34%)]\tLoss: 2.435175\n",
      "Train Epoch: 1 [20550/60000 (34%)]\tLoss: 2.871177\n",
      "Train Epoch: 1 [20600/60000 (34%)]\tLoss: 3.215697\n",
      "Train Epoch: 1 [20650/60000 (34%)]\tLoss: 2.715593\n",
      "Train Epoch: 1 [20700/60000 (34%)]\tLoss: 2.391090\n",
      "Train Epoch: 1 [20750/60000 (35%)]\tLoss: 2.936190\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 2.746412\n",
      "Train Epoch: 1 [20850/60000 (35%)]\tLoss: 2.523521\n",
      "Train Epoch: 1 [20900/60000 (35%)]\tLoss: 2.676506\n",
      "Train Epoch: 1 [20950/60000 (35%)]\tLoss: 2.041889\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 1.679370\n",
      "Train Epoch: 1 [21050/60000 (35%)]\tLoss: 1.657403\n",
      "Train Epoch: 1 [21100/60000 (35%)]\tLoss: 2.879678\n",
      "Train Epoch: 1 [21150/60000 (35%)]\tLoss: 2.662930\n",
      "Train Epoch: 1 [21200/60000 (35%)]\tLoss: 2.276088\n",
      "Train Epoch: 1 [21250/60000 (35%)]\tLoss: 3.335429\n",
      "Train Epoch: 1 [21300/60000 (36%)]\tLoss: 3.235678\n",
      "Train Epoch: 1 [21350/60000 (36%)]\tLoss: 1.991736\n",
      "Train Epoch: 1 [21400/60000 (36%)]\tLoss: 2.996937\n",
      "Train Epoch: 1 [21450/60000 (36%)]\tLoss: 3.726124\n",
      "Train Epoch: 1 [21500/60000 (36%)]\tLoss: 1.974050\n",
      "Train Epoch: 1 [21550/60000 (36%)]\tLoss: 3.434311\n",
      "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 1.988468\n",
      "Train Epoch: 1 [21650/60000 (36%)]\tLoss: 1.968188\n",
      "Train Epoch: 1 [21700/60000 (36%)]\tLoss: 1.803097\n",
      "Train Epoch: 1 [21750/60000 (36%)]\tLoss: 1.844609\n",
      "Train Epoch: 1 [21800/60000 (36%)]\tLoss: 2.603141\n",
      "Train Epoch: 1 [21850/60000 (36%)]\tLoss: 2.185358\n",
      "Train Epoch: 1 [21900/60000 (36%)]\tLoss: 2.221081\n",
      "Train Epoch: 1 [21950/60000 (37%)]\tLoss: 2.147660\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.875621\n",
      "Train Epoch: 1 [22050/60000 (37%)]\tLoss: 2.115974\n",
      "Train Epoch: 1 [22100/60000 (37%)]\tLoss: 3.002158\n",
      "Train Epoch: 1 [22150/60000 (37%)]\tLoss: 2.359240\n",
      "Train Epoch: 1 [22200/60000 (37%)]\tLoss: 3.482011\n",
      "Train Epoch: 1 [22250/60000 (37%)]\tLoss: 1.340441\n",
      "Train Epoch: 1 [22300/60000 (37%)]\tLoss: 2.771299\n",
      "Train Epoch: 1 [22350/60000 (37%)]\tLoss: 1.911901\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.284325\n",
      "Train Epoch: 1 [22450/60000 (37%)]\tLoss: 2.846385\n",
      "Train Epoch: 1 [22500/60000 (38%)]\tLoss: 1.979861\n",
      "Train Epoch: 1 [22550/60000 (38%)]\tLoss: 3.032265\n",
      "Train Epoch: 1 [22600/60000 (38%)]\tLoss: 1.883042\n",
      "Train Epoch: 1 [22650/60000 (38%)]\tLoss: 2.831995\n",
      "Train Epoch: 1 [22700/60000 (38%)]\tLoss: 3.174838\n",
      "Train Epoch: 1 [22750/60000 (38%)]\tLoss: 3.314041\n",
      "Train Epoch: 1 [22800/60000 (38%)]\tLoss: 2.304665\n",
      "Train Epoch: 1 [22850/60000 (38%)]\tLoss: 2.047844\n",
      "Train Epoch: 1 [22900/60000 (38%)]\tLoss: 2.577785\n",
      "Train Epoch: 1 [22950/60000 (38%)]\tLoss: 3.057417\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 2.930829\n",
      "Train Epoch: 1 [23050/60000 (38%)]\tLoss: 2.644503\n",
      "Train Epoch: 1 [23100/60000 (38%)]\tLoss: 1.929443\n",
      "Train Epoch: 1 [23150/60000 (39%)]\tLoss: 1.320732\n",
      "Train Epoch: 1 [23200/60000 (39%)]\tLoss: 1.815613\n",
      "Train Epoch: 1 [23250/60000 (39%)]\tLoss: 1.633226\n",
      "Train Epoch: 1 [23300/60000 (39%)]\tLoss: 2.318883\n",
      "Train Epoch: 1 [23350/60000 (39%)]\tLoss: 2.012027\n",
      "Train Epoch: 1 [23400/60000 (39%)]\tLoss: 2.142031\n",
      "Train Epoch: 1 [23450/60000 (39%)]\tLoss: 3.377820\n",
      "Train Epoch: 1 [23500/60000 (39%)]\tLoss: 2.706497\n",
      "Train Epoch: 1 [23550/60000 (39%)]\tLoss: 2.755761\n",
      "Train Epoch: 1 [23600/60000 (39%)]\tLoss: 2.382686\n",
      "Train Epoch: 1 [23650/60000 (39%)]\tLoss: 2.118055\n",
      "Train Epoch: 1 [23700/60000 (40%)]\tLoss: 2.688732\n",
      "Train Epoch: 1 [23750/60000 (40%)]\tLoss: 1.991815\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tLoss: 3.267507\n",
      "Train Epoch: 1 [23850/60000 (40%)]\tLoss: 3.040770\n",
      "Train Epoch: 1 [23900/60000 (40%)]\tLoss: 1.518947\n",
      "Train Epoch: 1 [23950/60000 (40%)]\tLoss: 2.594572\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 3.534454\n",
      "Train Epoch: 1 [24050/60000 (40%)]\tLoss: 2.772554\n",
      "Train Epoch: 1 [24100/60000 (40%)]\tLoss: 3.616367\n",
      "Train Epoch: 1 [24150/60000 (40%)]\tLoss: 2.900755\n",
      "Train Epoch: 1 [24200/60000 (40%)]\tLoss: 2.573677\n",
      "Train Epoch: 1 [24250/60000 (40%)]\tLoss: 1.878379\n",
      "Train Epoch: 1 [24300/60000 (40%)]\tLoss: 2.605597\n",
      "Train Epoch: 1 [24350/60000 (41%)]\tLoss: 2.564704\n",
      "Train Epoch: 1 [24400/60000 (41%)]\tLoss: 3.354654\n",
      "Train Epoch: 1 [24450/60000 (41%)]\tLoss: 1.436836\n",
      "Train Epoch: 1 [24500/60000 (41%)]\tLoss: 3.736742\n",
      "Train Epoch: 1 [24550/60000 (41%)]\tLoss: 4.059558\n",
      "Train Epoch: 1 [24600/60000 (41%)]\tLoss: 2.380996\n",
      "Train Epoch: 1 [24650/60000 (41%)]\tLoss: 2.515551\n",
      "Train Epoch: 1 [24700/60000 (41%)]\tLoss: 2.722622\n",
      "Train Epoch: 1 [24750/60000 (41%)]\tLoss: 3.044858\n",
      "Train Epoch: 1 [24800/60000 (41%)]\tLoss: 2.894621\n",
      "Train Epoch: 1 [24850/60000 (41%)]\tLoss: 1.467556\n",
      "Train Epoch: 1 [24900/60000 (42%)]\tLoss: 2.355987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [24950/60000 (42%)]\tLoss: 1.025915\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 3.192042\n",
      "Train Epoch: 1 [25050/60000 (42%)]\tLoss: 1.594761\n",
      "Train Epoch: 1 [25100/60000 (42%)]\tLoss: 2.488214\n",
      "Train Epoch: 1 [25150/60000 (42%)]\tLoss: 1.745486\n",
      "Train Epoch: 1 [25200/60000 (42%)]\tLoss: 2.707242\n",
      "Train Epoch: 1 [25250/60000 (42%)]\tLoss: 2.295367\n",
      "Train Epoch: 1 [25300/60000 (42%)]\tLoss: 2.240561\n",
      "Train Epoch: 1 [25350/60000 (42%)]\tLoss: 2.361475\n",
      "Train Epoch: 1 [25400/60000 (42%)]\tLoss: 3.128645\n",
      "Train Epoch: 1 [25450/60000 (42%)]\tLoss: 1.637009\n",
      "Train Epoch: 1 [25500/60000 (42%)]\tLoss: 2.799720\n",
      "Train Epoch: 1 [25550/60000 (43%)]\tLoss: 2.683041\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.839473\n",
      "Train Epoch: 1 [25650/60000 (43%)]\tLoss: 2.174706\n",
      "Train Epoch: 1 [25700/60000 (43%)]\tLoss: 1.169433\n",
      "Train Epoch: 1 [25750/60000 (43%)]\tLoss: 2.983115\n",
      "Train Epoch: 1 [25800/60000 (43%)]\tLoss: 2.934541\n",
      "Train Epoch: 1 [25850/60000 (43%)]\tLoss: 2.576325\n",
      "Train Epoch: 1 [25900/60000 (43%)]\tLoss: 2.578900\n",
      "Train Epoch: 1 [25950/60000 (43%)]\tLoss: 1.389624\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 4.700664\n",
      "Train Epoch: 1 [26050/60000 (43%)]\tLoss: 1.556476\n",
      "Train Epoch: 1 [26100/60000 (44%)]\tLoss: 2.276562\n",
      "Train Epoch: 1 [26150/60000 (44%)]\tLoss: 2.414174\n",
      "Train Epoch: 1 [26200/60000 (44%)]\tLoss: 2.586244\n",
      "Train Epoch: 1 [26250/60000 (44%)]\tLoss: 1.701270\n",
      "Train Epoch: 1 [26300/60000 (44%)]\tLoss: 2.468597\n",
      "Train Epoch: 1 [26350/60000 (44%)]\tLoss: 2.550762\n",
      "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 1.874586\n",
      "Train Epoch: 1 [26450/60000 (44%)]\tLoss: 2.928958\n",
      "Train Epoch: 1 [26500/60000 (44%)]\tLoss: 2.894486\n",
      "Train Epoch: 1 [26550/60000 (44%)]\tLoss: 2.745171\n",
      "Train Epoch: 1 [26600/60000 (44%)]\tLoss: 3.177506\n",
      "Train Epoch: 1 [26650/60000 (44%)]\tLoss: 4.360344\n",
      "Train Epoch: 1 [26700/60000 (44%)]\tLoss: 3.492053\n",
      "Train Epoch: 1 [26750/60000 (45%)]\tLoss: 2.861056\n",
      "Train Epoch: 1 [26800/60000 (45%)]\tLoss: 2.113660\n",
      "Train Epoch: 1 [26850/60000 (45%)]\tLoss: 2.768383\n",
      "Train Epoch: 1 [26900/60000 (45%)]\tLoss: 2.124244\n",
      "Train Epoch: 1 [26950/60000 (45%)]\tLoss: 1.042351\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 1.908936\n",
      "Train Epoch: 1 [27050/60000 (45%)]\tLoss: 2.395557\n",
      "Train Epoch: 1 [27100/60000 (45%)]\tLoss: 1.672093\n",
      "Train Epoch: 1 [27150/60000 (45%)]\tLoss: 1.851965\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 2.850693\n",
      "Train Epoch: 1 [27250/60000 (45%)]\tLoss: 1.724879\n",
      "Train Epoch: 1 [27300/60000 (46%)]\tLoss: 1.676751\n",
      "Train Epoch: 1 [27350/60000 (46%)]\tLoss: 2.120040\n",
      "Train Epoch: 1 [27400/60000 (46%)]\tLoss: 1.908929\n",
      "Train Epoch: 1 [27450/60000 (46%)]\tLoss: 2.883407\n",
      "Train Epoch: 1 [27500/60000 (46%)]\tLoss: 2.504524\n",
      "Train Epoch: 1 [27550/60000 (46%)]\tLoss: 2.634260\n",
      "Train Epoch: 1 [27600/60000 (46%)]\tLoss: 2.563550\n",
      "Train Epoch: 1 [27650/60000 (46%)]\tLoss: 2.819724\n",
      "Train Epoch: 1 [27700/60000 (46%)]\tLoss: 3.325914\n",
      "Train Epoch: 1 [27750/60000 (46%)]\tLoss: 2.913006\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tLoss: 3.363333\n",
      "Train Epoch: 1 [27850/60000 (46%)]\tLoss: 2.668177\n",
      "Train Epoch: 1 [27900/60000 (46%)]\tLoss: 2.458027\n",
      "Train Epoch: 1 [27950/60000 (47%)]\tLoss: 1.114004\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 2.193573\n",
      "Train Epoch: 1 [28050/60000 (47%)]\tLoss: 1.333077\n",
      "Train Epoch: 1 [28100/60000 (47%)]\tLoss: 2.469832\n",
      "Train Epoch: 1 [28150/60000 (47%)]\tLoss: 3.443277\n",
      "Train Epoch: 1 [28200/60000 (47%)]\tLoss: 1.414467\n",
      "Train Epoch: 1 [28250/60000 (47%)]\tLoss: 2.681833\n",
      "Train Epoch: 1 [28300/60000 (47%)]\tLoss: 1.699983\n",
      "Train Epoch: 1 [28350/60000 (47%)]\tLoss: 2.100726\n",
      "Train Epoch: 1 [28400/60000 (47%)]\tLoss: 2.302982\n",
      "Train Epoch: 1 [28450/60000 (47%)]\tLoss: 3.761910\n",
      "Train Epoch: 1 [28500/60000 (48%)]\tLoss: 2.298027\n",
      "Train Epoch: 1 [28550/60000 (48%)]\tLoss: 2.203446\n",
      "Train Epoch: 1 [28600/60000 (48%)]\tLoss: 3.514236\n",
      "Train Epoch: 1 [28650/60000 (48%)]\tLoss: 2.637675\n",
      "Train Epoch: 1 [28700/60000 (48%)]\tLoss: 3.223368\n",
      "Train Epoch: 1 [28750/60000 (48%)]\tLoss: 3.916103\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 3.380119\n",
      "Train Epoch: 1 [28850/60000 (48%)]\tLoss: 2.566472\n",
      "Train Epoch: 1 [28900/60000 (48%)]\tLoss: 2.611925\n",
      "Train Epoch: 1 [28950/60000 (48%)]\tLoss: 1.869190\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 1.813650\n",
      "Train Epoch: 1 [29050/60000 (48%)]\tLoss: 3.451466\n",
      "Train Epoch: 1 [29100/60000 (48%)]\tLoss: 2.591528\n",
      "Train Epoch: 1 [29150/60000 (49%)]\tLoss: 2.203152\n",
      "Train Epoch: 1 [29200/60000 (49%)]\tLoss: 0.778687\n",
      "Train Epoch: 1 [29250/60000 (49%)]\tLoss: 2.286096\n",
      "Train Epoch: 1 [29300/60000 (49%)]\tLoss: 2.734446\n",
      "Train Epoch: 1 [29350/60000 (49%)]\tLoss: 2.952475\n",
      "Train Epoch: 1 [29400/60000 (49%)]\tLoss: 2.250146\n",
      "Train Epoch: 1 [29450/60000 (49%)]\tLoss: 2.241780\n",
      "Train Epoch: 1 [29500/60000 (49%)]\tLoss: 2.538176\n",
      "Train Epoch: 1 [29550/60000 (49%)]\tLoss: 1.962671\n",
      "Train Epoch: 1 [29600/60000 (49%)]\tLoss: 2.801559\n",
      "Train Epoch: 1 [29650/60000 (49%)]\tLoss: 2.120966\n",
      "Train Epoch: 1 [29700/60000 (50%)]\tLoss: 2.949706\n",
      "Train Epoch: 1 [29750/60000 (50%)]\tLoss: 2.334238\n",
      "Train Epoch: 1 [29800/60000 (50%)]\tLoss: 2.350641\n",
      "Train Epoch: 1 [29850/60000 (50%)]\tLoss: 2.310106\n",
      "Train Epoch: 1 [29900/60000 (50%)]\tLoss: 3.197756\n",
      "Train Epoch: 1 [29950/60000 (50%)]\tLoss: 2.005265\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 3.055490\n",
      "Train Epoch: 1 [30050/60000 (50%)]\tLoss: 2.641052\n",
      "Train Epoch: 1 [30100/60000 (50%)]\tLoss: 2.435111\n",
      "Train Epoch: 1 [30150/60000 (50%)]\tLoss: 2.468503\n",
      "Train Epoch: 1 [30200/60000 (50%)]\tLoss: 2.238510\n",
      "Train Epoch: 1 [30250/60000 (50%)]\tLoss: 2.014769\n",
      "Train Epoch: 1 [30300/60000 (50%)]\tLoss: 2.688598\n",
      "Train Epoch: 1 [30350/60000 (51%)]\tLoss: 2.937102\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 2.445820\n",
      "Train Epoch: 1 [30450/60000 (51%)]\tLoss: 3.762786\n",
      "Train Epoch: 1 [30500/60000 (51%)]\tLoss: 1.306530\n",
      "Train Epoch: 1 [30550/60000 (51%)]\tLoss: 2.294282\n",
      "Train Epoch: 1 [30600/60000 (51%)]\tLoss: 1.609998\n",
      "Train Epoch: 1 [30650/60000 (51%)]\tLoss: 1.397870\n",
      "Train Epoch: 1 [30700/60000 (51%)]\tLoss: 3.009367\n",
      "Train Epoch: 1 [30750/60000 (51%)]\tLoss: 2.682681\n",
      "Train Epoch: 1 [30800/60000 (51%)]\tLoss: 3.343747\n",
      "Train Epoch: 1 [30850/60000 (51%)]\tLoss: 2.075289\n",
      "Train Epoch: 1 [30900/60000 (52%)]\tLoss: 3.508695\n",
      "Train Epoch: 1 [30950/60000 (52%)]\tLoss: 2.411090\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 2.699615\n",
      "Train Epoch: 1 [31050/60000 (52%)]\tLoss: 1.854038\n",
      "Train Epoch: 1 [31100/60000 (52%)]\tLoss: 2.693313\n",
      "Train Epoch: 1 [31150/60000 (52%)]\tLoss: 2.445702\n",
      "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 2.859308\n",
      "Train Epoch: 1 [31250/60000 (52%)]\tLoss: 2.987410\n",
      "Train Epoch: 1 [31300/60000 (52%)]\tLoss: 3.308129\n",
      "Train Epoch: 1 [31350/60000 (52%)]\tLoss: 2.402036\n",
      "Train Epoch: 1 [31400/60000 (52%)]\tLoss: 3.329301\n",
      "Train Epoch: 1 [31450/60000 (52%)]\tLoss: 2.221923\n",
      "Train Epoch: 1 [31500/60000 (52%)]\tLoss: 4.130424\n",
      "Train Epoch: 1 [31550/60000 (53%)]\tLoss: 4.164355\n",
      "Train Epoch: 1 [31600/60000 (53%)]\tLoss: 2.963461\n",
      "Train Epoch: 1 [31650/60000 (53%)]\tLoss: 1.142462\n",
      "Train Epoch: 1 [31700/60000 (53%)]\tLoss: 2.709737\n",
      "Train Epoch: 1 [31750/60000 (53%)]\tLoss: 3.669438\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tLoss: 2.008952\n",
      "Train Epoch: 1 [31850/60000 (53%)]\tLoss: 1.927518\n",
      "Train Epoch: 1 [31900/60000 (53%)]\tLoss: 1.965492\n",
      "Train Epoch: 1 [31950/60000 (53%)]\tLoss: 4.099812\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.262376\n",
      "Train Epoch: 1 [32050/60000 (53%)]\tLoss: 2.285793\n",
      "Train Epoch: 1 [32100/60000 (54%)]\tLoss: 2.094885\n",
      "Train Epoch: 1 [32150/60000 (54%)]\tLoss: 2.692445\n",
      "Train Epoch: 1 [32200/60000 (54%)]\tLoss: 3.452125\n",
      "Train Epoch: 1 [32250/60000 (54%)]\tLoss: 2.089430\n",
      "Train Epoch: 1 [32300/60000 (54%)]\tLoss: 3.116012\n",
      "Train Epoch: 1 [32350/60000 (54%)]\tLoss: 3.198021\n",
      "Train Epoch: 1 [32400/60000 (54%)]\tLoss: 2.164918\n",
      "Train Epoch: 1 [32450/60000 (54%)]\tLoss: 2.939095\n",
      "Train Epoch: 1 [32500/60000 (54%)]\tLoss: 2.127181\n",
      "Train Epoch: 1 [32550/60000 (54%)]\tLoss: 2.545742\n",
      "Train Epoch: 1 [32600/60000 (54%)]\tLoss: 1.354568\n",
      "Train Epoch: 1 [32650/60000 (54%)]\tLoss: 1.140557\n",
      "Train Epoch: 1 [32700/60000 (54%)]\tLoss: 2.883971\n",
      "Train Epoch: 1 [32750/60000 (55%)]\tLoss: 4.121161\n",
      "Train Epoch: 1 [32800/60000 (55%)]\tLoss: 2.528174\n",
      "Train Epoch: 1 [32850/60000 (55%)]\tLoss: 1.492899\n",
      "Train Epoch: 1 [32900/60000 (55%)]\tLoss: 2.886107\n",
      "Train Epoch: 1 [32950/60000 (55%)]\tLoss: 2.334237\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 3.547733\n",
      "Train Epoch: 1 [33050/60000 (55%)]\tLoss: 1.579903\n",
      "Train Epoch: 1 [33100/60000 (55%)]\tLoss: 2.393007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [33150/60000 (55%)]\tLoss: 3.615031\n",
      "Train Epoch: 1 [33200/60000 (55%)]\tLoss: 2.838298\n",
      "Train Epoch: 1 [33250/60000 (55%)]\tLoss: 3.124187\n",
      "Train Epoch: 1 [33300/60000 (56%)]\tLoss: 3.467735\n",
      "Train Epoch: 1 [33350/60000 (56%)]\tLoss: 4.048872\n",
      "Train Epoch: 1 [33400/60000 (56%)]\tLoss: 3.882712\n",
      "Train Epoch: 1 [33450/60000 (56%)]\tLoss: 2.440131\n",
      "Train Epoch: 1 [33500/60000 (56%)]\tLoss: 2.080773\n",
      "Train Epoch: 1 [33550/60000 (56%)]\tLoss: 2.754134\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 3.702142\n",
      "Train Epoch: 1 [33650/60000 (56%)]\tLoss: 1.982768\n",
      "Train Epoch: 1 [33700/60000 (56%)]\tLoss: 1.167317\n",
      "Train Epoch: 1 [33750/60000 (56%)]\tLoss: 3.942066\n",
      "Train Epoch: 1 [33800/60000 (56%)]\tLoss: 2.945643\n",
      "Train Epoch: 1 [33850/60000 (56%)]\tLoss: 2.370112\n",
      "Train Epoch: 1 [33900/60000 (56%)]\tLoss: 3.253590\n",
      "Train Epoch: 1 [33950/60000 (57%)]\tLoss: 2.533374\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 2.632288\n",
      "Train Epoch: 1 [34050/60000 (57%)]\tLoss: 2.795201\n",
      "Train Epoch: 1 [34100/60000 (57%)]\tLoss: 2.383190\n",
      "Train Epoch: 1 [34150/60000 (57%)]\tLoss: 3.110850\n",
      "Train Epoch: 1 [34200/60000 (57%)]\tLoss: 3.266553\n",
      "Train Epoch: 1 [34250/60000 (57%)]\tLoss: 3.471639\n",
      "Train Epoch: 1 [34300/60000 (57%)]\tLoss: 1.742511\n",
      "Train Epoch: 1 [34350/60000 (57%)]\tLoss: 2.449455\n",
      "Train Epoch: 1 [34400/60000 (57%)]\tLoss: 1.453682\n",
      "Train Epoch: 1 [34450/60000 (57%)]\tLoss: 2.581782\n",
      "Train Epoch: 1 [34500/60000 (58%)]\tLoss: 2.609293\n",
      "Train Epoch: 1 [34550/60000 (58%)]\tLoss: 3.874977\n",
      "Train Epoch: 1 [34600/60000 (58%)]\tLoss: 2.908380\n",
      "Train Epoch: 1 [34650/60000 (58%)]\tLoss: 1.459162\n",
      "Train Epoch: 1 [34700/60000 (58%)]\tLoss: 1.620810\n",
      "Train Epoch: 1 [34750/60000 (58%)]\tLoss: 3.406771\n",
      "Train Epoch: 1 [34800/60000 (58%)]\tLoss: 3.319671\n",
      "Train Epoch: 1 [34850/60000 (58%)]\tLoss: 2.876463\n",
      "Train Epoch: 1 [34900/60000 (58%)]\tLoss: 2.734437\n",
      "Train Epoch: 1 [34950/60000 (58%)]\tLoss: 3.481520\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 2.330430\n",
      "Train Epoch: 1 [35050/60000 (58%)]\tLoss: 3.834260\n",
      "Train Epoch: 1 [35100/60000 (58%)]\tLoss: 3.447428\n",
      "Train Epoch: 1 [35150/60000 (59%)]\tLoss: 1.319188\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 3.160223\n",
      "Train Epoch: 1 [35250/60000 (59%)]\tLoss: 2.970187\n",
      "Train Epoch: 1 [35300/60000 (59%)]\tLoss: 2.447218\n",
      "Train Epoch: 1 [35350/60000 (59%)]\tLoss: 2.651409\n",
      "Train Epoch: 1 [35400/60000 (59%)]\tLoss: 3.207098\n",
      "Train Epoch: 1 [35450/60000 (59%)]\tLoss: 1.983766\n",
      "Train Epoch: 1 [35500/60000 (59%)]\tLoss: 1.995515\n",
      "Train Epoch: 1 [35550/60000 (59%)]\tLoss: 2.520167\n",
      "Train Epoch: 1 [35600/60000 (59%)]\tLoss: 2.571854\n",
      "Train Epoch: 1 [35650/60000 (59%)]\tLoss: 3.136911\n",
      "Train Epoch: 1 [35700/60000 (60%)]\tLoss: 2.582473\n",
      "Train Epoch: 1 [35750/60000 (60%)]\tLoss: 2.223695\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tLoss: 2.481955\n",
      "Train Epoch: 1 [35850/60000 (60%)]\tLoss: 1.687219\n",
      "Train Epoch: 1 [35900/60000 (60%)]\tLoss: 2.796285\n",
      "Train Epoch: 1 [35950/60000 (60%)]\tLoss: 2.714648\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 2.850073\n",
      "Train Epoch: 1 [36050/60000 (60%)]\tLoss: 3.139875\n",
      "Train Epoch: 1 [36100/60000 (60%)]\tLoss: 2.198615\n",
      "Train Epoch: 1 [36150/60000 (60%)]\tLoss: 2.829486\n",
      "Train Epoch: 1 [36200/60000 (60%)]\tLoss: 2.766810\n",
      "Train Epoch: 1 [36250/60000 (60%)]\tLoss: 3.974790\n",
      "Train Epoch: 1 [36300/60000 (60%)]\tLoss: 2.632896\n",
      "Train Epoch: 1 [36350/60000 (61%)]\tLoss: 2.789245\n",
      "Train Epoch: 1 [36400/60000 (61%)]\tLoss: 3.931129\n",
      "Train Epoch: 1 [36450/60000 (61%)]\tLoss: 2.740145\n",
      "Train Epoch: 1 [36500/60000 (61%)]\tLoss: 3.437970\n",
      "Train Epoch: 1 [36550/60000 (61%)]\tLoss: 2.677030\n",
      "Train Epoch: 1 [36600/60000 (61%)]\tLoss: 2.125397\n",
      "Train Epoch: 1 [36650/60000 (61%)]\tLoss: 1.945215\n",
      "Train Epoch: 1 [36700/60000 (61%)]\tLoss: 3.434126\n",
      "Train Epoch: 1 [36750/60000 (61%)]\tLoss: 1.682802\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 3.853563\n",
      "Train Epoch: 1 [36850/60000 (61%)]\tLoss: 2.839019\n",
      "Train Epoch: 1 [36900/60000 (62%)]\tLoss: 3.658962\n",
      "Train Epoch: 1 [36950/60000 (62%)]\tLoss: 3.495737\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 2.184324\n",
      "Train Epoch: 1 [37050/60000 (62%)]\tLoss: 2.400689\n",
      "Train Epoch: 1 [37100/60000 (62%)]\tLoss: 4.094814\n",
      "Train Epoch: 1 [37150/60000 (62%)]\tLoss: 2.458365\n",
      "Train Epoch: 1 [37200/60000 (62%)]\tLoss: 2.621152\n",
      "Train Epoch: 1 [37250/60000 (62%)]\tLoss: 2.033018\n",
      "Train Epoch: 1 [37300/60000 (62%)]\tLoss: 2.474826\n",
      "Train Epoch: 1 [37350/60000 (62%)]\tLoss: 2.493461\n",
      "Train Epoch: 1 [37400/60000 (62%)]\tLoss: 4.178225\n",
      "Train Epoch: 1 [37450/60000 (62%)]\tLoss: 2.931097\n",
      "Train Epoch: 1 [37500/60000 (62%)]\tLoss: 1.885687\n",
      "Train Epoch: 1 [37550/60000 (63%)]\tLoss: 3.605879\n",
      "Train Epoch: 1 [37600/60000 (63%)]\tLoss: 2.410745\n",
      "Train Epoch: 1 [37650/60000 (63%)]\tLoss: 3.140828\n",
      "Train Epoch: 1 [37700/60000 (63%)]\tLoss: 3.435876\n",
      "Train Epoch: 1 [37750/60000 (63%)]\tLoss: 2.901398\n",
      "Train Epoch: 1 [37800/60000 (63%)]\tLoss: 3.340406\n",
      "Train Epoch: 1 [37850/60000 (63%)]\tLoss: 3.241781\n",
      "Train Epoch: 1 [37900/60000 (63%)]\tLoss: 3.373663\n",
      "Train Epoch: 1 [37950/60000 (63%)]\tLoss: 3.611580\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 2.693517\n",
      "Train Epoch: 1 [38050/60000 (63%)]\tLoss: 3.883839\n",
      "Train Epoch: 1 [38100/60000 (64%)]\tLoss: 5.087809\n",
      "Train Epoch: 1 [38150/60000 (64%)]\tLoss: 1.474128\n",
      "Train Epoch: 1 [38200/60000 (64%)]\tLoss: 1.125212\n",
      "Train Epoch: 1 [38250/60000 (64%)]\tLoss: 1.911774\n",
      "Train Epoch: 1 [38300/60000 (64%)]\tLoss: 3.407133\n",
      "Train Epoch: 1 [38350/60000 (64%)]\tLoss: 3.638845\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.250185\n",
      "Train Epoch: 1 [38450/60000 (64%)]\tLoss: 2.330984\n",
      "Train Epoch: 1 [38500/60000 (64%)]\tLoss: 4.527645\n",
      "Train Epoch: 1 [38550/60000 (64%)]\tLoss: 3.278796\n",
      "Train Epoch: 1 [38600/60000 (64%)]\tLoss: 2.895231\n",
      "Train Epoch: 1 [38650/60000 (64%)]\tLoss: 2.211870\n",
      "Train Epoch: 1 [38700/60000 (64%)]\tLoss: 4.123488\n",
      "Train Epoch: 1 [38750/60000 (65%)]\tLoss: 2.627267\n",
      "Train Epoch: 1 [38800/60000 (65%)]\tLoss: 2.106461\n",
      "Train Epoch: 1 [38850/60000 (65%)]\tLoss: 3.307392\n",
      "Train Epoch: 1 [38900/60000 (65%)]\tLoss: 2.197161\n",
      "Train Epoch: 1 [38950/60000 (65%)]\tLoss: 2.767176\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 3.355934\n",
      "Train Epoch: 1 [39050/60000 (65%)]\tLoss: 3.253069\n",
      "Train Epoch: 1 [39100/60000 (65%)]\tLoss: 2.928741\n",
      "Train Epoch: 1 [39150/60000 (65%)]\tLoss: 2.506716\n",
      "Train Epoch: 1 [39200/60000 (65%)]\tLoss: 3.253179\n",
      "Train Epoch: 1 [39250/60000 (65%)]\tLoss: 2.780880\n",
      "Train Epoch: 1 [39300/60000 (66%)]\tLoss: 3.409129\n",
      "Train Epoch: 1 [39350/60000 (66%)]\tLoss: 4.030336\n",
      "Train Epoch: 1 [39400/60000 (66%)]\tLoss: 1.851038\n",
      "Train Epoch: 1 [39450/60000 (66%)]\tLoss: 3.269977\n",
      "Train Epoch: 1 [39500/60000 (66%)]\tLoss: 1.662773\n",
      "Train Epoch: 1 [39550/60000 (66%)]\tLoss: 2.635429\n",
      "Train Epoch: 1 [39600/60000 (66%)]\tLoss: 2.762714\n",
      "Train Epoch: 1 [39650/60000 (66%)]\tLoss: 4.201479\n",
      "Train Epoch: 1 [39700/60000 (66%)]\tLoss: 1.814062\n",
      "Train Epoch: 1 [39750/60000 (66%)]\tLoss: 2.232425\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tLoss: 2.221553\n",
      "Train Epoch: 1 [39850/60000 (66%)]\tLoss: 2.407366\n",
      "Train Epoch: 1 [39900/60000 (66%)]\tLoss: 2.438550\n",
      "Train Epoch: 1 [39950/60000 (67%)]\tLoss: 2.066236\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 3.094389\n",
      "Train Epoch: 1 [40050/60000 (67%)]\tLoss: 4.732188\n",
      "Train Epoch: 1 [40100/60000 (67%)]\tLoss: 2.852818\n",
      "Train Epoch: 1 [40150/60000 (67%)]\tLoss: 2.487004\n",
      "Train Epoch: 1 [40200/60000 (67%)]\tLoss: 3.193991\n",
      "Train Epoch: 1 [40250/60000 (67%)]\tLoss: 2.522090\n",
      "Train Epoch: 1 [40300/60000 (67%)]\tLoss: 3.402336\n",
      "Train Epoch: 1 [40350/60000 (67%)]\tLoss: 3.208430\n",
      "Train Epoch: 1 [40400/60000 (67%)]\tLoss: 1.444203\n",
      "Train Epoch: 1 [40450/60000 (67%)]\tLoss: 3.445500\n",
      "Train Epoch: 1 [40500/60000 (68%)]\tLoss: 5.125680\n",
      "Train Epoch: 1 [40550/60000 (68%)]\tLoss: 2.656258\n",
      "Train Epoch: 1 [40600/60000 (68%)]\tLoss: 2.861361\n",
      "Train Epoch: 1 [40650/60000 (68%)]\tLoss: 3.355518\n",
      "Train Epoch: 1 [40700/60000 (68%)]\tLoss: 2.645312\n",
      "Train Epoch: 1 [40750/60000 (68%)]\tLoss: 3.057960\n",
      "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 2.100315\n",
      "Train Epoch: 1 [40850/60000 (68%)]\tLoss: 2.097057\n",
      "Train Epoch: 1 [40900/60000 (68%)]\tLoss: 2.946843\n",
      "Train Epoch: 1 [40950/60000 (68%)]\tLoss: 2.867330\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 5.507739\n",
      "Train Epoch: 1 [41050/60000 (68%)]\tLoss: 3.709733\n",
      "Train Epoch: 1 [41100/60000 (68%)]\tLoss: 2.858080\n",
      "Train Epoch: 1 [41150/60000 (69%)]\tLoss: 4.151896\n",
      "Train Epoch: 1 [41200/60000 (69%)]\tLoss: 1.820182\n",
      "Train Epoch: 1 [41250/60000 (69%)]\tLoss: 2.985255\n",
      "Train Epoch: 1 [41300/60000 (69%)]\tLoss: 1.734444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [41350/60000 (69%)]\tLoss: 4.147037\n",
      "Train Epoch: 1 [41400/60000 (69%)]\tLoss: 3.082244\n",
      "Train Epoch: 1 [41450/60000 (69%)]\tLoss: 1.371300\n",
      "Train Epoch: 1 [41500/60000 (69%)]\tLoss: 2.614472\n",
      "Train Epoch: 1 [41550/60000 (69%)]\tLoss: 3.886782\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.369542\n",
      "Train Epoch: 1 [41650/60000 (69%)]\tLoss: 3.256238\n",
      "Train Epoch: 1 [41700/60000 (70%)]\tLoss: 4.037656\n",
      "Train Epoch: 1 [41750/60000 (70%)]\tLoss: 2.304949\n",
      "Train Epoch: 1 [41800/60000 (70%)]\tLoss: 3.137400\n",
      "Train Epoch: 1 [41850/60000 (70%)]\tLoss: 3.373992\n",
      "Train Epoch: 1 [41900/60000 (70%)]\tLoss: 1.769817\n",
      "Train Epoch: 1 [41950/60000 (70%)]\tLoss: 3.215611\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 4.380465\n",
      "Train Epoch: 1 [42050/60000 (70%)]\tLoss: 4.722750\n",
      "Train Epoch: 1 [42100/60000 (70%)]\tLoss: 2.685276\n",
      "Train Epoch: 1 [42150/60000 (70%)]\tLoss: 2.288316\n",
      "Train Epoch: 1 [42200/60000 (70%)]\tLoss: 1.007621\n",
      "Train Epoch: 1 [42250/60000 (70%)]\tLoss: 2.703808\n",
      "Train Epoch: 1 [42300/60000 (70%)]\tLoss: 1.667388\n",
      "Train Epoch: 1 [42350/60000 (71%)]\tLoss: 2.950432\n",
      "Train Epoch: 1 [42400/60000 (71%)]\tLoss: 1.840013\n",
      "Train Epoch: 1 [42450/60000 (71%)]\tLoss: 1.891806\n",
      "Train Epoch: 1 [42500/60000 (71%)]\tLoss: 2.645193\n",
      "Train Epoch: 1 [42550/60000 (71%)]\tLoss: 3.661917\n",
      "Train Epoch: 1 [42600/60000 (71%)]\tLoss: 1.845168\n",
      "Train Epoch: 1 [42650/60000 (71%)]\tLoss: 4.581124\n",
      "Train Epoch: 1 [42700/60000 (71%)]\tLoss: 2.768610\n",
      "Train Epoch: 1 [42750/60000 (71%)]\tLoss: 3.264349\n",
      "Train Epoch: 1 [42800/60000 (71%)]\tLoss: 1.708669\n",
      "Train Epoch: 1 [42850/60000 (71%)]\tLoss: 2.340957\n",
      "Train Epoch: 1 [42900/60000 (72%)]\tLoss: 2.144966\n",
      "Train Epoch: 1 [42950/60000 (72%)]\tLoss: 3.712379\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 2.963110\n",
      "Train Epoch: 1 [43050/60000 (72%)]\tLoss: 3.064017\n",
      "Train Epoch: 1 [43100/60000 (72%)]\tLoss: 1.608852\n",
      "Train Epoch: 1 [43150/60000 (72%)]\tLoss: 3.888128\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 2.640703\n",
      "Train Epoch: 1 [43250/60000 (72%)]\tLoss: 3.201572\n",
      "Train Epoch: 1 [43300/60000 (72%)]\tLoss: 4.079025\n",
      "Train Epoch: 1 [43350/60000 (72%)]\tLoss: 3.060474\n",
      "Train Epoch: 1 [43400/60000 (72%)]\tLoss: 3.402604\n",
      "Train Epoch: 1 [43450/60000 (72%)]\tLoss: 3.344299\n",
      "Train Epoch: 1 [43500/60000 (72%)]\tLoss: 2.002866\n",
      "Train Epoch: 1 [43550/60000 (73%)]\tLoss: 2.401093\n",
      "Train Epoch: 1 [43600/60000 (73%)]\tLoss: 2.572405\n",
      "Train Epoch: 1 [43650/60000 (73%)]\tLoss: 2.120924\n",
      "Train Epoch: 1 [43700/60000 (73%)]\tLoss: 1.606573\n",
      "Train Epoch: 1 [43750/60000 (73%)]\tLoss: 3.488191\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tLoss: 3.078481\n",
      "Train Epoch: 1 [43850/60000 (73%)]\tLoss: 3.699405\n",
      "Train Epoch: 1 [43900/60000 (73%)]\tLoss: 3.770442\n",
      "Train Epoch: 1 [43950/60000 (73%)]\tLoss: 3.305855\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 2.398019\n",
      "Train Epoch: 1 [44050/60000 (73%)]\tLoss: 2.730680\n",
      "Train Epoch: 1 [44100/60000 (74%)]\tLoss: 2.865674\n",
      "Train Epoch: 1 [44150/60000 (74%)]\tLoss: 2.805610\n",
      "Train Epoch: 1 [44200/60000 (74%)]\tLoss: 2.899810\n",
      "Train Epoch: 1 [44250/60000 (74%)]\tLoss: 3.168340\n",
      "Train Epoch: 1 [44300/60000 (74%)]\tLoss: 0.715281\n",
      "Train Epoch: 1 [44350/60000 (74%)]\tLoss: 3.498480\n",
      "Train Epoch: 1 [44400/60000 (74%)]\tLoss: 2.656271\n",
      "Train Epoch: 1 [44450/60000 (74%)]\tLoss: 3.333069\n",
      "Train Epoch: 1 [44500/60000 (74%)]\tLoss: 2.592270\n",
      "Train Epoch: 1 [44550/60000 (74%)]\tLoss: 2.715048\n",
      "Train Epoch: 1 [44600/60000 (74%)]\tLoss: 4.006771\n",
      "Train Epoch: 1 [44650/60000 (74%)]\tLoss: 3.848130\n",
      "Train Epoch: 1 [44700/60000 (74%)]\tLoss: 3.476562\n",
      "Train Epoch: 1 [44750/60000 (75%)]\tLoss: 2.700206\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.842698\n",
      "Train Epoch: 1 [44850/60000 (75%)]\tLoss: 4.374671\n",
      "Train Epoch: 1 [44900/60000 (75%)]\tLoss: 2.002420\n",
      "Train Epoch: 1 [44950/60000 (75%)]\tLoss: 4.139366\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 3.864881\n",
      "Train Epoch: 1 [45050/60000 (75%)]\tLoss: 1.642116\n",
      "Train Epoch: 1 [45100/60000 (75%)]\tLoss: 2.550567\n",
      "Train Epoch: 1 [45150/60000 (75%)]\tLoss: 3.125264\n",
      "Train Epoch: 1 [45200/60000 (75%)]\tLoss: 3.717716\n",
      "Train Epoch: 1 [45250/60000 (75%)]\tLoss: 3.147351\n",
      "Train Epoch: 1 [45300/60000 (76%)]\tLoss: 3.737853\n",
      "Train Epoch: 1 [45350/60000 (76%)]\tLoss: 2.453997\n",
      "Train Epoch: 1 [45400/60000 (76%)]\tLoss: 1.706625\n",
      "Train Epoch: 1 [45450/60000 (76%)]\tLoss: 2.350069\n",
      "Train Epoch: 1 [45500/60000 (76%)]\tLoss: 4.102197\n",
      "Train Epoch: 1 [45550/60000 (76%)]\tLoss: 3.193881\n",
      "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 1.412085\n",
      "Train Epoch: 1 [45650/60000 (76%)]\tLoss: 2.474843\n",
      "Train Epoch: 1 [45700/60000 (76%)]\tLoss: 3.107537\n",
      "Train Epoch: 1 [45750/60000 (76%)]\tLoss: 1.536750\n",
      "Train Epoch: 1 [45800/60000 (76%)]\tLoss: 2.916734\n",
      "Train Epoch: 1 [45850/60000 (76%)]\tLoss: 3.144804\n",
      "Train Epoch: 1 [45900/60000 (76%)]\tLoss: 1.831166\n",
      "Train Epoch: 1 [45950/60000 (77%)]\tLoss: 4.285415\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 3.316575\n",
      "Train Epoch: 1 [46050/60000 (77%)]\tLoss: 4.086993\n",
      "Train Epoch: 1 [46100/60000 (77%)]\tLoss: 4.322656\n",
      "Train Epoch: 1 [46150/60000 (77%)]\tLoss: 2.407824\n",
      "Train Epoch: 1 [46200/60000 (77%)]\tLoss: 1.851440\n",
      "Train Epoch: 1 [46250/60000 (77%)]\tLoss: 2.795765\n",
      "Train Epoch: 1 [46300/60000 (77%)]\tLoss: 2.756695\n",
      "Train Epoch: 1 [46350/60000 (77%)]\tLoss: 4.024301\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 3.360618\n",
      "Train Epoch: 1 [46450/60000 (77%)]\tLoss: 3.416888\n",
      "Train Epoch: 1 [46500/60000 (78%)]\tLoss: 3.484987\n",
      "Train Epoch: 1 [46550/60000 (78%)]\tLoss: 2.661736\n",
      "Train Epoch: 1 [46600/60000 (78%)]\tLoss: 5.928622\n",
      "Train Epoch: 1 [46650/60000 (78%)]\tLoss: 1.911193\n",
      "Train Epoch: 1 [46700/60000 (78%)]\tLoss: 1.943532\n",
      "Train Epoch: 1 [46750/60000 (78%)]\tLoss: 4.269891\n",
      "Train Epoch: 1 [46800/60000 (78%)]\tLoss: 3.359365\n",
      "Train Epoch: 1 [46850/60000 (78%)]\tLoss: 1.971174\n",
      "Train Epoch: 1 [46900/60000 (78%)]\tLoss: 2.042797\n",
      "Train Epoch: 1 [46950/60000 (78%)]\tLoss: 1.663500\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 2.363153\n",
      "Train Epoch: 1 [47050/60000 (78%)]\tLoss: 2.147386\n",
      "Train Epoch: 1 [47100/60000 (78%)]\tLoss: 4.395247\n",
      "Train Epoch: 1 [47150/60000 (79%)]\tLoss: 2.610703\n",
      "Train Epoch: 1 [47200/60000 (79%)]\tLoss: 3.643569\n",
      "Train Epoch: 1 [47250/60000 (79%)]\tLoss: 3.764850\n",
      "Train Epoch: 1 [47300/60000 (79%)]\tLoss: 2.649582\n",
      "Train Epoch: 1 [47350/60000 (79%)]\tLoss: 2.975804\n",
      "Train Epoch: 1 [47400/60000 (79%)]\tLoss: 2.854321\n",
      "Train Epoch: 1 [47450/60000 (79%)]\tLoss: 2.928159\n",
      "Train Epoch: 1 [47500/60000 (79%)]\tLoss: 4.263698\n",
      "Train Epoch: 1 [47550/60000 (79%)]\tLoss: 4.298298\n",
      "Train Epoch: 1 [47600/60000 (79%)]\tLoss: 4.070615\n",
      "Train Epoch: 1 [47650/60000 (79%)]\tLoss: 2.675951\n",
      "Train Epoch: 1 [47700/60000 (80%)]\tLoss: 3.016816\n",
      "Train Epoch: 1 [47750/60000 (80%)]\tLoss: 3.554113\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tLoss: 3.238679\n",
      "Train Epoch: 1 [47850/60000 (80%)]\tLoss: 2.930084\n",
      "Train Epoch: 1 [47900/60000 (80%)]\tLoss: 2.766871\n",
      "Train Epoch: 1 [47950/60000 (80%)]\tLoss: 3.080862\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.857038\n",
      "Train Epoch: 1 [48050/60000 (80%)]\tLoss: 2.218158\n",
      "Train Epoch: 1 [48100/60000 (80%)]\tLoss: 3.153500\n",
      "Train Epoch: 1 [48150/60000 (80%)]\tLoss: 5.040162\n",
      "Train Epoch: 1 [48200/60000 (80%)]\tLoss: 3.662871\n",
      "Train Epoch: 1 [48250/60000 (80%)]\tLoss: 1.760616\n",
      "Train Epoch: 1 [48300/60000 (80%)]\tLoss: 2.485190\n",
      "Train Epoch: 1 [48350/60000 (81%)]\tLoss: 2.128281\n",
      "Train Epoch: 1 [48400/60000 (81%)]\tLoss: 5.295770\n",
      "Train Epoch: 1 [48450/60000 (81%)]\tLoss: 3.786545\n",
      "Train Epoch: 1 [48500/60000 (81%)]\tLoss: 2.346874\n",
      "Train Epoch: 1 [48550/60000 (81%)]\tLoss: 3.299535\n",
      "Train Epoch: 1 [48600/60000 (81%)]\tLoss: 1.848082\n",
      "Train Epoch: 1 [48650/60000 (81%)]\tLoss: 3.939018\n",
      "Train Epoch: 1 [48700/60000 (81%)]\tLoss: 1.450250\n",
      "Train Epoch: 1 [48750/60000 (81%)]\tLoss: 2.899164\n",
      "Train Epoch: 1 [48800/60000 (81%)]\tLoss: 2.728134\n",
      "Train Epoch: 1 [48850/60000 (81%)]\tLoss: 3.813385\n",
      "Train Epoch: 1 [48900/60000 (82%)]\tLoss: 2.895460\n",
      "Train Epoch: 1 [48950/60000 (82%)]\tLoss: 3.772926\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 5.140316\n",
      "Train Epoch: 1 [49050/60000 (82%)]\tLoss: 1.868895\n",
      "Train Epoch: 1 [49100/60000 (82%)]\tLoss: 2.444150\n",
      "Train Epoch: 1 [49150/60000 (82%)]\tLoss: 3.677209\n",
      "Train Epoch: 1 [49200/60000 (82%)]\tLoss: 1.998125\n",
      "Train Epoch: 1 [49250/60000 (82%)]\tLoss: 2.901773\n",
      "Train Epoch: 1 [49300/60000 (82%)]\tLoss: 2.146887\n",
      "Train Epoch: 1 [49350/60000 (82%)]\tLoss: 2.287362\n",
      "Train Epoch: 1 [49400/60000 (82%)]\tLoss: 3.797237\n",
      "Train Epoch: 1 [49450/60000 (82%)]\tLoss: 3.583315\n",
      "Train Epoch: 1 [49500/60000 (82%)]\tLoss: 2.140085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [49550/60000 (83%)]\tLoss: 2.216639\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 2.981045\n",
      "Train Epoch: 1 [49650/60000 (83%)]\tLoss: 2.657011\n",
      "Train Epoch: 1 [49700/60000 (83%)]\tLoss: 4.977817\n",
      "Train Epoch: 1 [49750/60000 (83%)]\tLoss: 3.629218\n",
      "Train Epoch: 1 [49800/60000 (83%)]\tLoss: 4.793130\n",
      "Train Epoch: 1 [49850/60000 (83%)]\tLoss: 2.925933\n",
      "Train Epoch: 1 [49900/60000 (83%)]\tLoss: 3.106675\n",
      "Train Epoch: 1 [49950/60000 (83%)]\tLoss: 5.104890\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 3.062166\n",
      "Train Epoch: 1 [50050/60000 (83%)]\tLoss: 3.499479\n",
      "Train Epoch: 1 [50100/60000 (84%)]\tLoss: 2.443806\n",
      "Train Epoch: 1 [50150/60000 (84%)]\tLoss: 2.446835\n",
      "Train Epoch: 1 [50200/60000 (84%)]\tLoss: 1.262188\n",
      "Train Epoch: 1 [50250/60000 (84%)]\tLoss: 2.558575\n",
      "Train Epoch: 1 [50300/60000 (84%)]\tLoss: 5.074514\n",
      "Train Epoch: 1 [50350/60000 (84%)]\tLoss: 4.099364\n",
      "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 2.181319\n",
      "Train Epoch: 1 [50450/60000 (84%)]\tLoss: 4.502198\n",
      "Train Epoch: 1 [50500/60000 (84%)]\tLoss: 2.280692\n",
      "Train Epoch: 1 [50550/60000 (84%)]\tLoss: 2.391914\n",
      "Train Epoch: 1 [50600/60000 (84%)]\tLoss: 2.417057\n",
      "Train Epoch: 1 [50650/60000 (84%)]\tLoss: 2.116349\n",
      "Train Epoch: 1 [50700/60000 (84%)]\tLoss: 2.891369\n",
      "Train Epoch: 1 [50750/60000 (85%)]\tLoss: 2.833698\n",
      "Train Epoch: 1 [50800/60000 (85%)]\tLoss: 2.947128\n",
      "Train Epoch: 1 [50850/60000 (85%)]\tLoss: 2.233671\n",
      "Train Epoch: 1 [50900/60000 (85%)]\tLoss: 3.364663\n",
      "Train Epoch: 1 [50950/60000 (85%)]\tLoss: 2.423703\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 4.451163\n",
      "Train Epoch: 1 [51050/60000 (85%)]\tLoss: 5.247758\n",
      "Train Epoch: 1 [51100/60000 (85%)]\tLoss: 2.863209\n",
      "Train Epoch: 1 [51150/60000 (85%)]\tLoss: 2.304438\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 3.206523\n",
      "Train Epoch: 1 [51250/60000 (85%)]\tLoss: 3.101188\n",
      "Train Epoch: 1 [51300/60000 (86%)]\tLoss: 2.049594\n",
      "Train Epoch: 1 [51350/60000 (86%)]\tLoss: 2.934808\n",
      "Train Epoch: 1 [51400/60000 (86%)]\tLoss: 3.678879\n",
      "Train Epoch: 1 [51450/60000 (86%)]\tLoss: 2.764316\n",
      "Train Epoch: 1 [51500/60000 (86%)]\tLoss: 2.527831\n",
      "Train Epoch: 1 [51550/60000 (86%)]\tLoss: 2.269967\n",
      "Train Epoch: 1 [51600/60000 (86%)]\tLoss: 2.658084\n",
      "Train Epoch: 1 [51650/60000 (86%)]\tLoss: 2.733591\n",
      "Train Epoch: 1 [51700/60000 (86%)]\tLoss: 2.625364\n",
      "Train Epoch: 1 [51750/60000 (86%)]\tLoss: 2.462705\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tLoss: 4.267301\n",
      "Train Epoch: 1 [51850/60000 (86%)]\tLoss: 5.065749\n",
      "Train Epoch: 1 [51900/60000 (86%)]\tLoss: 1.771878\n",
      "Train Epoch: 1 [51950/60000 (87%)]\tLoss: 3.502154\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 2.353332\n",
      "Train Epoch: 1 [52050/60000 (87%)]\tLoss: 3.169320\n",
      "Train Epoch: 1 [52100/60000 (87%)]\tLoss: 3.774062\n",
      "Train Epoch: 1 [52150/60000 (87%)]\tLoss: 2.776242\n",
      "Train Epoch: 1 [52200/60000 (87%)]\tLoss: 2.653553\n",
      "Train Epoch: 1 [52250/60000 (87%)]\tLoss: 1.284970\n",
      "Train Epoch: 1 [52300/60000 (87%)]\tLoss: 2.535299\n",
      "Train Epoch: 1 [52350/60000 (87%)]\tLoss: 2.292540\n",
      "Train Epoch: 1 [52400/60000 (87%)]\tLoss: 2.584763\n",
      "Train Epoch: 1 [52450/60000 (87%)]\tLoss: 2.598614\n",
      "Train Epoch: 1 [52500/60000 (88%)]\tLoss: 3.826996\n",
      "Train Epoch: 1 [52550/60000 (88%)]\tLoss: 2.731730\n",
      "Train Epoch: 1 [52600/60000 (88%)]\tLoss: 3.898359\n",
      "Train Epoch: 1 [52650/60000 (88%)]\tLoss: 1.808479\n",
      "Train Epoch: 1 [52700/60000 (88%)]\tLoss: 4.412618\n",
      "Train Epoch: 1 [52750/60000 (88%)]\tLoss: 3.152045\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 4.011582\n",
      "Train Epoch: 1 [52850/60000 (88%)]\tLoss: 3.007905\n",
      "Train Epoch: 1 [52900/60000 (88%)]\tLoss: 2.962842\n",
      "Train Epoch: 1 [52950/60000 (88%)]\tLoss: 3.354607\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 4.197970\n",
      "Train Epoch: 1 [53050/60000 (88%)]\tLoss: 2.255557\n",
      "Train Epoch: 1 [53100/60000 (88%)]\tLoss: 2.366754\n",
      "Train Epoch: 1 [53150/60000 (89%)]\tLoss: 5.013133\n",
      "Train Epoch: 1 [53200/60000 (89%)]\tLoss: 3.684776\n",
      "Train Epoch: 1 [53250/60000 (89%)]\tLoss: 3.490139\n",
      "Train Epoch: 1 [53300/60000 (89%)]\tLoss: 2.370632\n",
      "Train Epoch: 1 [53350/60000 (89%)]\tLoss: 4.531803\n",
      "Train Epoch: 1 [53400/60000 (89%)]\tLoss: 3.517971\n",
      "Train Epoch: 1 [53450/60000 (89%)]\tLoss: 3.365218\n",
      "Train Epoch: 1 [53500/60000 (89%)]\tLoss: 4.640807\n",
      "Train Epoch: 1 [53550/60000 (89%)]\tLoss: 3.241047\n",
      "Train Epoch: 1 [53600/60000 (89%)]\tLoss: 1.254477\n",
      "Train Epoch: 1 [53650/60000 (89%)]\tLoss: 2.656784\n",
      "Train Epoch: 1 [53700/60000 (90%)]\tLoss: 3.471637\n",
      "Train Epoch: 1 [53750/60000 (90%)]\tLoss: 3.732194\n",
      "Train Epoch: 1 [53800/60000 (90%)]\tLoss: 2.564199\n",
      "Train Epoch: 1 [53850/60000 (90%)]\tLoss: 3.585057\n",
      "Train Epoch: 1 [53900/60000 (90%)]\tLoss: 1.376688\n",
      "Train Epoch: 1 [53950/60000 (90%)]\tLoss: 5.329929\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 2.480942\n",
      "Train Epoch: 1 [54050/60000 (90%)]\tLoss: 4.651966\n",
      "Train Epoch: 1 [54100/60000 (90%)]\tLoss: 3.543488\n",
      "Train Epoch: 1 [54150/60000 (90%)]\tLoss: 2.762915\n",
      "Train Epoch: 1 [54200/60000 (90%)]\tLoss: 3.416948\n",
      "Train Epoch: 1 [54250/60000 (90%)]\tLoss: 3.669935\n",
      "Train Epoch: 1 [54300/60000 (90%)]\tLoss: 3.113180\n",
      "Train Epoch: 1 [54350/60000 (91%)]\tLoss: 3.130732\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 4.032630\n",
      "Train Epoch: 1 [54450/60000 (91%)]\tLoss: 2.961300\n",
      "Train Epoch: 1 [54500/60000 (91%)]\tLoss: 3.637154\n",
      "Train Epoch: 1 [54550/60000 (91%)]\tLoss: 4.408707\n",
      "Train Epoch: 1 [54600/60000 (91%)]\tLoss: 2.756203\n",
      "Train Epoch: 1 [54650/60000 (91%)]\tLoss: 2.755447\n",
      "Train Epoch: 1 [54700/60000 (91%)]\tLoss: 3.404085\n",
      "Train Epoch: 1 [54750/60000 (91%)]\tLoss: 2.565869\n",
      "Train Epoch: 1 [54800/60000 (91%)]\tLoss: 3.671331\n",
      "Train Epoch: 1 [54850/60000 (91%)]\tLoss: 2.828292\n",
      "Train Epoch: 1 [54900/60000 (92%)]\tLoss: 4.403255\n",
      "Train Epoch: 1 [54950/60000 (92%)]\tLoss: 2.517118\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 2.440177\n",
      "Train Epoch: 1 [55050/60000 (92%)]\tLoss: 2.580323\n",
      "Train Epoch: 1 [55100/60000 (92%)]\tLoss: 5.508775\n",
      "Train Epoch: 1 [55150/60000 (92%)]\tLoss: 2.242472\n",
      "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 2.990563\n",
      "Train Epoch: 1 [55250/60000 (92%)]\tLoss: 3.768702\n",
      "Train Epoch: 1 [55300/60000 (92%)]\tLoss: 4.660937\n",
      "Train Epoch: 1 [55350/60000 (92%)]\tLoss: 1.873403\n",
      "Train Epoch: 1 [55400/60000 (92%)]\tLoss: 2.441603\n",
      "Train Epoch: 1 [55450/60000 (92%)]\tLoss: 4.012363\n",
      "Train Epoch: 1 [55500/60000 (92%)]\tLoss: 2.115047\n",
      "Train Epoch: 1 [55550/60000 (93%)]\tLoss: 2.787478\n",
      "Train Epoch: 1 [55600/60000 (93%)]\tLoss: 2.192152\n",
      "Train Epoch: 1 [55650/60000 (93%)]\tLoss: 2.567348\n",
      "Train Epoch: 1 [55700/60000 (93%)]\tLoss: 2.969834\n",
      "Train Epoch: 1 [55750/60000 (93%)]\tLoss: 3.560508\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tLoss: 3.383903\n",
      "Train Epoch: 1 [55850/60000 (93%)]\tLoss: 4.188314\n",
      "Train Epoch: 1 [55900/60000 (93%)]\tLoss: 3.199741\n",
      "Train Epoch: 1 [55950/60000 (93%)]\tLoss: 3.735431\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 3.978663\n",
      "Train Epoch: 1 [56050/60000 (93%)]\tLoss: 3.089427\n",
      "Train Epoch: 1 [56100/60000 (94%)]\tLoss: 3.640906\n",
      "Train Epoch: 1 [56150/60000 (94%)]\tLoss: 2.599294\n",
      "Train Epoch: 1 [56200/60000 (94%)]\tLoss: 2.592302\n",
      "Train Epoch: 1 [56250/60000 (94%)]\tLoss: 2.919790\n",
      "Train Epoch: 1 [56300/60000 (94%)]\tLoss: 2.874856\n",
      "Train Epoch: 1 [56350/60000 (94%)]\tLoss: 2.878278\n",
      "Train Epoch: 1 [56400/60000 (94%)]\tLoss: 3.777618\n",
      "Train Epoch: 1 [56450/60000 (94%)]\tLoss: 4.405899\n",
      "Train Epoch: 1 [56500/60000 (94%)]\tLoss: 3.233547\n",
      "Train Epoch: 1 [56550/60000 (94%)]\tLoss: 1.767244\n",
      "Train Epoch: 1 [56600/60000 (94%)]\tLoss: 4.435661\n",
      "Train Epoch: 1 [56650/60000 (94%)]\tLoss: 4.630105\n",
      "Train Epoch: 1 [56700/60000 (94%)]\tLoss: 3.214705\n",
      "Train Epoch: 1 [56750/60000 (95%)]\tLoss: 3.594536\n",
      "Train Epoch: 1 [56800/60000 (95%)]\tLoss: 2.104305\n",
      "Train Epoch: 1 [56850/60000 (95%)]\tLoss: 2.720748\n",
      "Train Epoch: 1 [56900/60000 (95%)]\tLoss: 3.429398\n",
      "Train Epoch: 1 [56950/60000 (95%)]\tLoss: 3.016525\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 3.651080\n",
      "Train Epoch: 1 [57050/60000 (95%)]\tLoss: 3.869718\n",
      "Train Epoch: 1 [57100/60000 (95%)]\tLoss: 3.736343\n",
      "Train Epoch: 1 [57150/60000 (95%)]\tLoss: 3.760687\n",
      "Train Epoch: 1 [57200/60000 (95%)]\tLoss: 4.093797\n",
      "Train Epoch: 1 [57250/60000 (95%)]\tLoss: 2.778216\n",
      "Train Epoch: 1 [57300/60000 (96%)]\tLoss: 2.508782\n",
      "Train Epoch: 1 [57350/60000 (96%)]\tLoss: 4.063023\n",
      "Train Epoch: 1 [57400/60000 (96%)]\tLoss: 5.746612\n",
      "Train Epoch: 1 [57450/60000 (96%)]\tLoss: 1.125554\n",
      "Train Epoch: 1 [57500/60000 (96%)]\tLoss: 3.534848\n",
      "Train Epoch: 1 [57550/60000 (96%)]\tLoss: 3.625726\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.427261\n",
      "Train Epoch: 1 [57650/60000 (96%)]\tLoss: 3.108585\n",
      "Train Epoch: 1 [57700/60000 (96%)]\tLoss: 3.911017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [57750/60000 (96%)]\tLoss: 3.082692\n",
      "Train Epoch: 1 [57800/60000 (96%)]\tLoss: 3.690936\n",
      "Train Epoch: 1 [57850/60000 (96%)]\tLoss: 2.969610\n",
      "Train Epoch: 1 [57900/60000 (96%)]\tLoss: 5.056574\n",
      "Train Epoch: 1 [57950/60000 (97%)]\tLoss: 5.559714\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 3.002325\n",
      "Train Epoch: 1 [58050/60000 (97%)]\tLoss: 2.142268\n",
      "Train Epoch: 1 [58100/60000 (97%)]\tLoss: 2.567738\n",
      "Train Epoch: 1 [58150/60000 (97%)]\tLoss: 2.750252\n",
      "Train Epoch: 1 [58200/60000 (97%)]\tLoss: 1.399550\n",
      "Train Epoch: 1 [58250/60000 (97%)]\tLoss: 3.518047\n",
      "Train Epoch: 1 [58300/60000 (97%)]\tLoss: 2.731811\n",
      "Train Epoch: 1 [58350/60000 (97%)]\tLoss: 2.653485\n",
      "Train Epoch: 1 [58400/60000 (97%)]\tLoss: 2.188731\n",
      "Train Epoch: 1 [58450/60000 (97%)]\tLoss: 4.024837\n",
      "Train Epoch: 1 [58500/60000 (98%)]\tLoss: 2.996370\n",
      "Train Epoch: 1 [58550/60000 (98%)]\tLoss: 2.909110\n",
      "Train Epoch: 1 [58600/60000 (98%)]\tLoss: 3.732906\n",
      "Train Epoch: 1 [58650/60000 (98%)]\tLoss: 3.111708\n",
      "Train Epoch: 1 [58700/60000 (98%)]\tLoss: 2.886787\n",
      "Train Epoch: 1 [58750/60000 (98%)]\tLoss: 4.061242\n",
      "Train Epoch: 1 [58800/60000 (98%)]\tLoss: 2.450104\n",
      "Train Epoch: 1 [58850/60000 (98%)]\tLoss: 3.313202\n",
      "Train Epoch: 1 [58900/60000 (98%)]\tLoss: 2.932069\n",
      "Train Epoch: 1 [58950/60000 (98%)]\tLoss: 2.273766\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 1.706328\n",
      "Train Epoch: 1 [59050/60000 (98%)]\tLoss: 4.787211\n",
      "Train Epoch: 1 [59100/60000 (98%)]\tLoss: 5.301868\n",
      "Train Epoch: 1 [59150/60000 (99%)]\tLoss: 4.171434\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 1.797493\n",
      "Train Epoch: 1 [59250/60000 (99%)]\tLoss: 4.378610\n",
      "Train Epoch: 1 [59300/60000 (99%)]\tLoss: 2.436332\n",
      "Train Epoch: 1 [59350/60000 (99%)]\tLoss: 2.787061\n",
      "Train Epoch: 1 [59400/60000 (99%)]\tLoss: 3.076156\n",
      "Train Epoch: 1 [59450/60000 (99%)]\tLoss: 3.371464\n",
      "Train Epoch: 1 [59500/60000 (99%)]\tLoss: 2.652377\n",
      "Train Epoch: 1 [59550/60000 (99%)]\tLoss: 3.925060\n",
      "Train Epoch: 1 [59600/60000 (99%)]\tLoss: 2.488729\n",
      "Train Epoch: 1 [59650/60000 (99%)]\tLoss: 1.824036\n",
      "Train Epoch: 1 [59700/60000 (100%)]\tLoss: 4.170667\n",
      "Train Epoch: 1 [59750/60000 (100%)]\tLoss: 1.772978\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tLoss: 1.833479\n",
      "Train Epoch: 1 [59850/60000 (100%)]\tLoss: 3.452518\n",
      "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 3.824355\n",
      "Train Epoch: 1 [59950/60000 (100%)]\tLoss: 2.589465\n"
     ]
    }
   ],
   "source": [
    "# local_dp = False\n",
    "# RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "log_interval = 5\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# model = Net()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "RPC_train(data, log_interval, local_dp, epoch, round)\n",
    "\n",
    "# RPC_test(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}