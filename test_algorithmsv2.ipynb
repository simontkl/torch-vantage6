{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the params (tensors)\n",
    "\n",
    "# model = Net()\n",
    "\n",
    "# for parameter in model.parameters():\n",
    "#     print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def RPC_initialize_training(data, gamma, learning_rate, local_dp):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # print(\"\\033[0;{};49m Rank {} is training on {}\".format(device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net().to(device)\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "\n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "                sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "                max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train(data, log_interval, local_dp, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        log_interval: The amount of rounds before logging intermediate loss.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "    \"\"\"\n",
    "    # loading arguments/parameters from first RPC_method\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp) # is this allowed in vantage6? calling one RPC_method in another?\n",
    "     \n",
    "    train_loader = data\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        # Send the data and target to the device (cpu/gpu) the model is at\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Clear gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model on the data\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Adding differential privacy or not\n",
    "    if local_dp == True:\n",
    "        epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "#             print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def RPC_test(data):\n",
    "    \"\"\"\n",
    "    Tests the model.\n",
    "\n",
    "    Args:\n",
    "        color: The color for the terminal output for this worker.\n",
    "        model: The model to test.\n",
    "        device: The device to test the model on.\n",
    "        test_loader: The local loader for test local. -> no inside function\n",
    "    \"\"\"\n",
    "\n",
    "    test_loader = data\n",
    "    \n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "     \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are the parameters needed for the function\n",
    "Data loading and transforming (this will be done beforehand \n",
    "and then stored in './local/training.pt' and './testing.pt')\n",
    "\"\"\"\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "log_interval=10 \n",
    "\n",
    "gamma=0.7\n",
    "\n",
    "\n",
    "\n",
    "# train_data = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "#                                                           download=True,\n",
    "#                                                           train=True,\n",
    "#                                                           transform=transforms.Compose([\n",
    "#                                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "#                                                               transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "#                                                           ])), \n",
    "#                                            batch_size=64, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_data = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "#                                                           download=True,\n",
    "#                                                           train=False,\n",
    "#                                                           transform=transforms.Compose([\n",
    "#                                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "#                                                               transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "#                                                           ])), \n",
    "#                                            batch_size=1000, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# torch.save(train_data, \"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "# torch.save(test_data, \"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\testing.pt\")\n",
    "\n",
    "\n",
    "train_loader = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "\n",
    "\n",
    "test_loader = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\testing.pt\")\n",
    "\n",
    "\n",
    "local_dp = False\n",
    "\n",
    "epoch = 3\n",
    "\n",
    "round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2987, Accuracy: 932/10000 (9%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.330241\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.077637\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.697539\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.159781\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 1.165870\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.898840\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.769878\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.557259\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 1.010401\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.561366\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.865985\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.672712\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.645665\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.599921\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.634895\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.336758\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.683337\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.808593\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.518190\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.551686\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.546423\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.636599\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.463512\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.737188\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.266453\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.403307\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.277818\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.618967\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.371422\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.456310\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.208502\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.431864\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.359351\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.626353\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.395534\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.494972\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.429074\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.439869\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.344861\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.317193\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.403773\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.408351\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.359401\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.285485\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.304937\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.381784\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.514138\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.294225\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.392769\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.345209\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.280414\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.281948\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.404142\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.219553\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.558930\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.227172\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.353201\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.300545\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.318865\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.267010\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.441050\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.224418\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.242556\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.427064\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.342690\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.472285\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.330950\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.356352\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.332032\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.223233\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.480177\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.293520\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.192808\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.108461\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.238019\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.236143\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.256470\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.407469\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.236208\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.306343\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.230063\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.305908\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.400085\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.273683\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.242178\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.298818\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.298080\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.259233\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.376217\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.325941\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.182178\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.408780\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.177150\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.393684\n",
      "\n",
      "Test set: Average loss: 2.3176, Accuracy: 743/10000 (7%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.330241\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.077631\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.696964\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.158604\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 1.163639\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.895308\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.756298\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.555307\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.992467\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.566727\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.849479\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.689216\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.671397\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.589032\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.607954\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.316484\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.664160\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.828094\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.476677\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.542217\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.551385\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.638320\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.444852\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.744951\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.262252\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.422678\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.271377\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.619823\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.407068\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.473802\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.228966\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.426499\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.362961\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.602796\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.378962\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.493910\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.418690\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.425370\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.334050\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.321714\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.403532\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.436941\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.391085\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.280258\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.283557\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.401770\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.500047\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.287528\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.406878\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.378763\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.259782\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.260609\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.368650\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.219637\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.460104\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.206411\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.343755\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.266884\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.320127\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.295045\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.445788\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.217046\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.255051\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.421833\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.320485\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.445037\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.317937\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.335137\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.298850\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.217645\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.517279\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.294599\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.201761\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.129224\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.262949\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.251735\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.283563\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.427649\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.230419\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.335521\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.239746\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.274919\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.427618\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.286561\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.242342\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.305927\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.290904\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.290397\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.341169\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.366265\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.168236\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.389158\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.177234\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.429436\n",
      "\n",
      "Test set: Average loss: 2.3176, Accuracy: 743/10000 (7%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.330241\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.077631\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.696975\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 1.158586\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 1.163819\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.898237\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.755919\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.562672\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.995325\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.554349\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.857663\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.699393\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.677606\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.575199\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.634462\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.321381\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.701648\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.798143\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.455550\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.540584\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.556076\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.636767\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.443278\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.747801\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.254289\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.398571\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.295553\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.585884\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.398240\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.517061\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.226983\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.418295\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.356327\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.581489\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.346623\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.527989\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.450433\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.404877\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.348099\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.307941\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.442331\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.416394\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.374543\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.281620\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.311838\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.409523\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.492407\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.295501\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.380090\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.377633\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.268454\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.253938\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.358484\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.213162\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.486005\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.234271\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.352896\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.265853\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.324842\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.306707\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.429555\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.234203\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.244278\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.414343\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.310589\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.443362\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.325465\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.353468\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.309098\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.220479\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.489661\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.293067\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.206472\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.127003\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.230198\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.236167\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.277622\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.425291\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.247270\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.355352\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.233724\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.285067\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.439937\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.293737\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.236729\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.291818\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.331277\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.289177\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.368875\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.333471\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.168827\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.400202\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.178960\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.407844\n",
      "\n",
      "Test set: Average loss: 2.3176, Accuracy: 743/10000 (7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "RPC_test(test_loader)\n",
    "for epoch in range(1, epoch + 1):\n",
    "    torch.manual_seed(1)\n",
    "    RPC_train(train_loader, 10, False, 3)\n",
    "    RPC_test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FED AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def RPC_get_parameters(data, model, parameters):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    data_size = len(data) // 3 # number of nodes# size of dataset\n",
    "    \n",
    "    RPC_train(data, log_interval, local_dp, epoch, round)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for parameters in model.parameters():\n",
    "            return {\"params\": parameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def average_parameters(data, model):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters(data, model, parameters) # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "\n",
    "    params = []\n",
    "    n_nodes = len(organizations)  # how many organisations?\n",
    "\n",
    "    for output in parameters:\n",
    "        parameters += output[\"parameters\"]\n",
    "\n",
    "    return {\"params_average\": parameters / n_nodes}\n",
    "\n",
    "    \n",
    "\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#     # The first entry of the provided parameters when using dist.gather\n",
    "#     # method also contains the value from the server, remove that one\n",
    "#     minus_server = parameters[i][1:]\n",
    "#     # Calculate the average by summing and dividing by the number of\n",
    "#     # workers\n",
    "#     s = sum(minus_server)\n",
    "#     average = s/len(minus_server)\n",
    "#     # Change the parameter of the global model to the average\n",
    "#     param.data = average\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "model = Net()\n",
    "parameters=model.parameters()\n",
    "learning_rate=0.01\n",
    "log_interval=10 \n",
    "gamma=0.7\n",
    "round = 1\n",
    "epoch =1\n",
    "\n",
    "RPC_get_parameters(data, model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_parameters(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, local_dp, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "    model = average_parameters(data, model)\n",
    "    \n",
    "    for epoch in range(1, epoch + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train(data, local_dp, model, device, optimizer, epoch, delta=1e-5)\n",
    "        # Test the model on the workers\n",
    "        RPC_test(data, model, device)\n",
    "\n",
    "    gather_params = model.get_parameters() # or model.parameters()\n",
    "\n",
    "    RPC_train(model.RPC_average_parameters_weighted(gather_params))\n",
    "\n",
    "    return model, parameters\n",
    "\n",
    "\n",
    "    ## OR \n",
    "\n",
    "#     parameters = RPC_average_parameters_weighted(data, model, parameters, weights) # then uses those parameters for training\n",
    "\n",
    "\n",
    "\n",
    "        # # Gather the parameters after the training round on the server\n",
    "        #     gather_params = coor.gather_parameters(rank, model, group_size + 1, subgroup)\n",
    "        #\n",
    "        #     # If the server\n",
    "        #     if rank == 0:\n",
    "        #         # Calculate the average of the parameters and adjust global model\n",
    "        #         coor.average_parameters_weighted(model, gather_params, weights)\n",
    "        #\n",
    "        #     # Send the new model parameters to the workers\n",
    "        #     coor.broadcast_parameters(model, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPC_fed_avg(data, local_dp, epoch, delta=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}