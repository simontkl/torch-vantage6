{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Own modules\n",
    "# import v6simplemodel as sm\n",
    "# import util.parser as parser\n",
    "import parser as parse\n",
    "# import db as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def RPC_initialize_training(data, gamma=0.7, learning_rate=0.01, local_dp=True):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    # Load local dataset\n",
    "    # first:\n",
    "        # torch.save(dataset_train, './dataset.pt')\n",
    "#     data = torch.load('./training.pt')\n",
    "\n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#     print(\"\\033[0;{};49m Rank {} is training on {}\".format(color, rank, device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # TODO: load local? train_loader, test_loader from locally stored local\n",
    "\n",
    "    # use Opacus for DP: Opacus is a library that enables training PyTorch models\n",
    "    # with differential privacy. Taken from: https://github.com/pytorch/opacus\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "            sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "            max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train(data, model, device, optimizer, epoch=1, delta=1e-5, local_dp=True):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        model: A model to run training on.\n",
    "        device: The device to run training on.\n",
    "        train_loader: Data loader for training local.\n",
    "        optim: Optimization algorithm used for training. (not optimizer because double use)\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "    \"\"\"\n",
    "    # TODO: define train_loader again from local local\n",
    "#     data = torch.load('./dataset.pt')\n",
    "#     train_loader, test_loader, data_size = data\n",
    "\n",
    "#     train = torch.load('/Users/simontokloth/PycharmProjects/torch-vantage6/v6-ppsdg-py/local/MNIST/processed/training.pt')\n",
    "#     test = torch.load('/Users/simontokloth/PyCharmProjects/torch-vantage6/v6-ppsdg-py/local/MNIST/processed/test.pt')\n",
    "#     train_loader, test_loader = train, test\n",
    "    \n",
    "#     transform = transforms.Compose([transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    device, model, optimizer = RPC_initialize_training(data) # is this allowed in vantage6? calling one RPC_method in another?\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#     for batch_idx, sample in enumerate(train_loader): \n",
    "#         data, target = sample['data'].cuda(), sample['target'].cuda()\n",
    "        \n",
    "        # Send the local and target to the device (cpu/gpu) the model is at (either send to the cpu or to the gpu, but the local is already on the worker node); model.send(local.location)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        batch = (data, target)\n",
    "        # Clear gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model on the local\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    # Logging needed if want the same output as torch.dist\n",
    "#     print('\\033[0;{};49m Train on Rank {}, Round {}, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#         round, epoch, batch_idx * len(batch[0]), len(train_loader.dataset),\n",
    "#         100. * batch_idx / len(train_loader), loss.item()))\n",
    "    print(loss.item())\n",
    "    \n",
    "#     optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "#     if local_dp==True:\n",
    "#         privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "#             sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "#             max_grad_norm=1.0,)\n",
    "#         privacy_engine.attach(optimizer)\n",
    "        \n",
    "    epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "#     print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def RPC_test(data, model, device):\n",
    "    \"\"\"\n",
    "    Tests the model.\n",
    "\n",
    "    Args:\n",
    "        color: The color for the terminal output for this worker.\n",
    "        model: The model to test.\n",
    "        device: The device to test the model on.\n",
    "        test_loader: The local loader for test local. -> no inside function\n",
    "    \"\"\"\n",
    "    # TODO: load local dataset as test_loader\n",
    "#     data = torch.load('./dataset.pt')\n",
    "#     train_loader, test_loader, data_size = data\n",
    "\n",
    "#     train = torch.load('/Users/simontokloth/PycharmProjects/torch-vantage6/v6-ppsdg-py/local/MNIST/processed/training.pt')\n",
    "#     test = torch.load('/Users/simontokloth/PyCharmProjects/torch-vantage6/v6-ppsdg-py/local/MNIST/processed/test.pt')\n",
    "#     train_loader, test_loader = train, test\n",
    "\n",
    "#     transform = transforms.Compose([transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "    \n",
    "    device, model, optimizer = RPC_initialize_training(data)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(test_loss)\n",
    "#     print('\\033[0;{};49m \\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def RPC_get_parameters(data, model, parameters, weights):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    for parameters in model.parameters():\n",
    "        return {\n",
    "        \"params\": parameters,\n",
    "        }\n",
    "    \n",
    "    '''\n",
    "    this might need to be combined with training, so that train returns the parameters or that it at least calls the results of training function\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def RPC_average_parameters_weighted(data, model, parameters, weights):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters() # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "    \n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for parameters in model.parameters():\n",
    "            average = sum(x * y for x, y in zip(parameters[i], weights)) / sum(weights)\n",
    "            parameters.data = average\n",
    "            i = i + 1\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, args, model, optimizer, train_loader, test_loader, device):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train(data, model, device, train_loader, optimizer, epoch, args.local_dp, delta=1e-5)\n",
    "        # Test the model on the workers\n",
    "        RPC_test(data, model, device, test_loader)\n",
    "\n",
    "    gather_params = model.get_parameters() # or model.parameters()\n",
    "\n",
    "    RPC_train(model.RPC_average_parameters_weighted(gather_params))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "## OR \n",
    "\n",
    "    parameters = RPC_average_parameters_weighted(data, model, parameters, weights) # then uses those parameters for training\n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Gather the parameters after the training round on the server\n",
    "    #     gather_params = coor.gather_parameters(rank, model, group_size + 1, subgroup)\n",
    "    #\n",
    "    #     # If the server\n",
    "    #     if rank == 0:\n",
    "    #         # Calculate the average of the parameters and adjust global model\n",
    "    #         coor.average_parameters_weighted(model, gather_params, weights)\n",
    "    #\n",
    "    #     # Send the new model parameters to the workers\n",
    "    #     coor.broadcast_parameters(model, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:518: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.035912990570068\n"
     ]
    }
   ],
   "source": [
    "# why do I have to specify these?\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "data = data_loader\n",
    "\n",
    "model = Net()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "# RPC_initialize_training(data)\n",
    "\n",
    "RPC_train(data, model, device, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:518: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3067076303482055\n"
     ]
    }
   ],
   "source": [
    "RPC_test(data, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}