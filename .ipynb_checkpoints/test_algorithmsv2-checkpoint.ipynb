{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0225, -0.1485,  0.2741],\n",
      "          [ 0.2235, -0.1770, -0.0121],\n",
      "          [ 0.0343,  0.2802, -0.2322]]],\n",
      "\n",
      "\n",
      "        [[[-0.0380,  0.3233, -0.1519],\n",
      "          [-0.1442, -0.2900,  0.0544],\n",
      "          [-0.0365, -0.0022,  0.2541]]],\n",
      "\n",
      "\n",
      "        [[[-0.0459, -0.0309, -0.1744],\n",
      "          [-0.2157, -0.0522,  0.0290],\n",
      "          [-0.2771,  0.0896,  0.0995]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2058, -0.1706,  0.1562],\n",
      "          [ 0.0351,  0.2632, -0.0909],\n",
      "          [ 0.0705,  0.0177, -0.1358]]],\n",
      "\n",
      "\n",
      "        [[[-0.2432, -0.2814, -0.2173],\n",
      "          [-0.0088,  0.1790, -0.1137],\n",
      "          [ 0.2901,  0.0789,  0.1269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3119, -0.0789, -0.0647],\n",
      "          [-0.1001,  0.2150,  0.2421],\n",
      "          [ 0.1643, -0.0379, -0.0938]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0360, -0.2663,  0.2820],\n",
      "          [-0.2712, -0.1065, -0.1080],\n",
      "          [ 0.2841,  0.0246,  0.1412]]],\n",
      "\n",
      "\n",
      "        [[[-0.2295, -0.2360,  0.2830],\n",
      "          [-0.0740,  0.2506,  0.3099],\n",
      "          [-0.1006,  0.1921,  0.2137]]],\n",
      "\n",
      "\n",
      "        [[[-0.1879, -0.2295,  0.2832],\n",
      "          [-0.0726,  0.1646,  0.0167],\n",
      "          [-0.1896,  0.0543,  0.0752]]],\n",
      "\n",
      "\n",
      "        [[[-0.0242,  0.1500,  0.0027],\n",
      "          [-0.1969, -0.2484,  0.3138],\n",
      "          [ 0.1352, -0.0091, -0.0579]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3253, -0.0121, -0.1976],\n",
      "          [-0.3210, -0.2878, -0.1655],\n",
      "          [-0.0136, -0.2554, -0.2155]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0946,  0.0959,  0.0494],\n",
      "          [-0.0287, -0.0971,  0.2317],\n",
      "          [-0.1609, -0.2309, -0.1909]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1656, -0.1873,  0.2854],\n",
      "          [-0.1911, -0.0860, -0.1761],\n",
      "          [ 0.0915, -0.2335,  0.1491]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1548, -0.0057, -0.1907],\n",
      "          [ 0.3256, -0.2506,  0.0797],\n",
      "          [ 0.1629, -0.2815, -0.0084]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2642, -0.0335,  0.0529],\n",
      "          [-0.1974, -0.1155, -0.2341],\n",
      "          [-0.0121,  0.1886, -0.1885]]],\n",
      "\n",
      "\n",
      "        [[[-0.1222, -0.1458,  0.2994],\n",
      "          [ 0.1476, -0.1134,  0.0370],\n",
      "          [ 0.2897, -0.0063, -0.3113]]],\n",
      "\n",
      "\n",
      "        [[[-0.2523,  0.1036, -0.1743],\n",
      "          [-0.1626, -0.0322, -0.3076],\n",
      "          [-0.2010, -0.1035,  0.1383]]],\n",
      "\n",
      "\n",
      "        [[[-0.2453, -0.2506,  0.2971],\n",
      "          [ 0.2817,  0.2766,  0.0004],\n",
      "          [-0.0687,  0.0725,  0.0309]]],\n",
      "\n",
      "\n",
      "        [[[-0.2842, -0.2834, -0.0908],\n",
      "          [ 0.0373, -0.2786,  0.1115],\n",
      "          [ 0.2505,  0.0908,  0.2845]]],\n",
      "\n",
      "\n",
      "        [[[-0.0105,  0.1809,  0.3155],\n",
      "          [-0.3228, -0.1533,  0.2173],\n",
      "          [-0.0689,  0.1371,  0.1522]]],\n",
      "\n",
      "\n",
      "        [[[-0.0673, -0.1596,  0.2093],\n",
      "          [-0.0027,  0.1324, -0.2106],\n",
      "          [-0.1847,  0.0857, -0.1982]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3170,  0.2525, -0.3062],\n",
      "          [ 0.0800,  0.1597, -0.1089],\n",
      "          [ 0.0983,  0.0007,  0.1904]]],\n",
      "\n",
      "\n",
      "        [[[-0.0869,  0.0433, -0.2139],\n",
      "          [-0.2676,  0.1483, -0.1930],\n",
      "          [ 0.1061,  0.0275,  0.1722]]],\n",
      "\n",
      "\n",
      "        [[[-0.2749, -0.1052,  0.3226],\n",
      "          [ 0.2313, -0.2152,  0.1053],\n",
      "          [ 0.2187,  0.0826, -0.2204]]],\n",
      "\n",
      "\n",
      "        [[[-0.0180, -0.2074,  0.2620],\n",
      "          [-0.0848, -0.1625,  0.3104],\n",
      "          [ 0.1670,  0.2705,  0.2044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2158, -0.2341,  0.1196],\n",
      "          [-0.0813,  0.2699,  0.1941],\n",
      "          [-0.3019, -0.1446, -0.0871]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0822, -0.1807, -0.3085],\n",
      "          [-0.0090,  0.1179, -0.0489],\n",
      "          [-0.1108,  0.2035,  0.0849]]],\n",
      "\n",
      "\n",
      "        [[[-0.1978,  0.1056,  0.2552],\n",
      "          [-0.2241, -0.2890, -0.2957],\n",
      "          [-0.2146, -0.0745, -0.2485]]],\n",
      "\n",
      "\n",
      "        [[[-0.1658,  0.2778,  0.2703],\n",
      "          [ 0.0411, -0.0347,  0.3165],\n",
      "          [-0.0822,  0.2422,  0.0315]]],\n",
      "\n",
      "\n",
      "        [[[-0.0472, -0.2094, -0.0684],\n",
      "          [-0.1671,  0.2642, -0.3179],\n",
      "          [-0.2979,  0.2171, -0.0304]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0646, -0.1053, -0.2105],\n",
      "          [ 0.0697,  0.3195,  0.1967],\n",
      "          [-0.1317,  0.1836,  0.2783]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2283, -0.0862, -0.0787],\n",
      "          [-0.1111, -0.1462, -0.1416],\n",
      "          [-0.0217,  0.1526,  0.1009]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3153, -0.2983, -0.2845,  0.2464, -0.3208, -0.0706,  0.2315,  0.2973,\n",
      "         0.1572, -0.0006,  0.1780,  0.2640,  0.2457,  0.0358, -0.1587, -0.2159,\n",
      "        -0.2830, -0.3149,  0.2691,  0.3045, -0.0586,  0.3032, -0.2348, -0.1358,\n",
      "         0.1754,  0.2786, -0.1775,  0.2028,  0.1125,  0.1290,  0.2000, -0.0159],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-4.9808e-02,  2.6025e-02, -1.3785e-02],\n",
      "          [ 5.6592e-02, -3.3250e-02,  3.7616e-02],\n",
      "          [-2.6543e-02,  3.2173e-02, -5.0083e-02]],\n",
      "\n",
      "         [[ 1.8831e-02,  4.5786e-03,  2.9074e-02],\n",
      "          [-4.5983e-02,  2.0050e-02, -1.2819e-02],\n",
      "          [-5.5721e-02,  2.3877e-02,  2.8131e-02]],\n",
      "\n",
      "         [[-9.4604e-03, -3.7356e-02,  1.6405e-02],\n",
      "          [ 1.7537e-02, -4.1146e-02,  4.1177e-03],\n",
      "          [-4.2003e-02,  4.8455e-02, -3.8726e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0755e-02,  3.6488e-02, -6.0832e-03],\n",
      "          [-5.1902e-02,  1.2124e-03, -2.1272e-02],\n",
      "          [ 2.0833e-02,  5.6412e-02,  1.5244e-04]],\n",
      "\n",
      "         [[ 1.8122e-02, -1.2031e-02, -4.7383e-02],\n",
      "          [-1.8303e-02, -1.4675e-02, -5.5349e-02],\n",
      "          [-3.6972e-02, -4.2843e-02,  5.3918e-02]],\n",
      "\n",
      "         [[-1.6756e-02,  5.0338e-02,  3.5804e-02],\n",
      "          [ 4.3493e-02,  4.5932e-02, -5.1145e-03],\n",
      "          [-1.3879e-02, -1.2268e-02, -5.0620e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5018e-02,  3.5066e-02, -5.8352e-02],\n",
      "          [-4.9282e-02, -5.7611e-02, -3.5756e-02],\n",
      "          [ 4.4081e-02, -1.9742e-02, -3.6748e-02]],\n",
      "\n",
      "         [[ 1.8126e-02, -1.6753e-03, -2.7906e-02],\n",
      "          [-3.3668e-02, -9.3533e-03,  5.1648e-02],\n",
      "          [ 2.5612e-02, -3.7687e-02, -5.2562e-02]],\n",
      "\n",
      "         [[-3.3956e-03, -5.4157e-02, -8.0933e-03],\n",
      "          [ 3.3888e-03, -5.3719e-04,  4.7624e-02],\n",
      "          [ 5.2634e-02, -4.5304e-02,  3.9998e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6308e-02,  5.1731e-02,  4.9126e-02],\n",
      "          [-3.7075e-02, -4.9437e-02,  2.5912e-02],\n",
      "          [-4.8042e-02,  2.4146e-03,  3.6740e-04]],\n",
      "\n",
      "         [[-7.6517e-03,  6.5510e-03, -3.5301e-02],\n",
      "          [-1.9901e-02, -3.2973e-02, -4.3905e-02],\n",
      "          [ 8.4859e-03, -2.0577e-03,  1.7736e-02]],\n",
      "\n",
      "         [[-4.5450e-02,  4.2505e-03,  3.3087e-02],\n",
      "          [ 5.2462e-02, -4.7526e-02,  4.4408e-02],\n",
      "          [ 3.9028e-02,  6.0047e-03, -1.5016e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5386e-02,  3.3401e-03, -3.1173e-02],\n",
      "          [-5.2486e-02,  3.1810e-02, -2.9948e-02],\n",
      "          [-1.7300e-02, -2.6391e-02,  9.4366e-03]],\n",
      "\n",
      "         [[ 2.6155e-02,  3.5589e-02,  7.6927e-03],\n",
      "          [ 5.6009e-02,  2.7680e-02,  3.8483e-02],\n",
      "          [ 3.0454e-02,  4.6418e-02,  5.5536e-02]],\n",
      "\n",
      "         [[-1.4670e-02,  2.2855e-02, -5.4700e-02],\n",
      "          [ 1.5396e-02, -2.9168e-02, -4.2642e-03],\n",
      "          [-5.4534e-02,  3.9395e-02, -5.9518e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.7874e-03,  4.8886e-02,  5.0682e-02],\n",
      "          [-5.7626e-02,  2.2525e-02, -2.9223e-02],\n",
      "          [-5.6954e-02, -4.3101e-02,  2.0354e-02]],\n",
      "\n",
      "         [[-4.5676e-02,  1.1670e-02, -1.9112e-02],\n",
      "          [ 4.7377e-02,  4.7782e-02,  9.7709e-04],\n",
      "          [-3.5830e-02, -5.8767e-02,  5.3129e-02]],\n",
      "\n",
      "         [[ 4.2487e-02, -7.2100e-03, -7.2006e-05],\n",
      "          [-1.4668e-02, -2.0404e-02,  4.9625e-02],\n",
      "          [-4.6416e-02, -1.8938e-02, -1.9205e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.7873e-02, -2.1243e-02,  4.4669e-02],\n",
      "          [ 2.4970e-02, -2.1197e-02,  5.6375e-02],\n",
      "          [-3.2177e-02,  3.8732e-02,  5.5813e-02]],\n",
      "\n",
      "         [[ 5.0098e-03, -3.9102e-02, -3.5597e-02],\n",
      "          [ 3.0075e-03,  6.7552e-03,  3.9041e-02],\n",
      "          [ 6.9764e-03,  1.5713e-03,  4.5113e-02]],\n",
      "\n",
      "         [[ 1.0916e-02, -3.5055e-02,  3.3079e-03],\n",
      "          [ 2.7088e-02, -4.9510e-03,  2.7568e-02],\n",
      "          [ 3.1346e-02, -4.4897e-02, -4.3555e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1295e-02, -2.3721e-02, -2.2010e-02],\n",
      "          [-3.7863e-02, -4.1670e-02, -3.3800e-02],\n",
      "          [ 3.6731e-02, -8.0729e-03,  5.2678e-02]],\n",
      "\n",
      "         [[-7.3992e-03,  2.5988e-02,  1.3403e-02],\n",
      "          [-5.5360e-02, -4.7264e-02, -8.9118e-04],\n",
      "          [ 4.6956e-02, -4.0712e-02, -5.0031e-02]],\n",
      "\n",
      "         [[-3.2603e-02,  5.7253e-02, -5.1026e-02],\n",
      "          [-4.6087e-02, -1.8686e-02, -1.0481e-02],\n",
      "          [ 9.3615e-03,  2.1621e-02,  9.3213e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3402e-03,  7.3937e-03,  3.0644e-02],\n",
      "          [ 4.0828e-02,  9.4154e-04,  2.8588e-02],\n",
      "          [-3.4902e-03, -3.9561e-02, -4.1982e-02]],\n",
      "\n",
      "         [[ 1.9698e-02, -5.5108e-02,  4.8671e-02],\n",
      "          [-3.6072e-02, -1.7549e-02,  6.4203e-04],\n",
      "          [ 5.1245e-02, -2.2359e-02,  4.3795e-02]],\n",
      "\n",
      "         [[-5.2951e-02, -2.3248e-02,  1.7912e-03],\n",
      "          [ 5.8759e-02, -4.7525e-02,  2.2722e-02],\n",
      "          [-3.6196e-02, -3.5608e-03, -2.3452e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9715e-02, -3.4635e-02,  3.9455e-02],\n",
      "          [-4.1380e-02, -4.5073e-02, -9.2005e-04],\n",
      "          [-4.1761e-02, -3.2278e-03,  7.2034e-03]],\n",
      "\n",
      "         [[-4.6889e-02, -2.4511e-02, -2.5604e-02],\n",
      "          [ 1.4067e-03, -4.5537e-02, -2.3826e-02],\n",
      "          [ 1.6997e-02,  1.1400e-02, -2.2838e-02]],\n",
      "\n",
      "         [[ 5.6612e-02, -4.2747e-03,  1.8137e-02],\n",
      "          [ 3.6597e-02, -1.1442e-02,  5.9846e-03],\n",
      "          [-4.3062e-02, -3.8433e-02, -4.6746e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9118e-02, -5.3770e-02, -3.3410e-02],\n",
      "          [ 1.6601e-02,  4.9150e-02, -2.7552e-02],\n",
      "          [-1.4619e-02, -2.4964e-02, -4.3018e-02]],\n",
      "\n",
      "         [[-3.8745e-02, -5.6563e-02,  5.8861e-02],\n",
      "          [ 2.2865e-03,  5.7806e-02,  4.5871e-02],\n",
      "          [-1.8649e-02, -2.2012e-02,  2.7221e-02]],\n",
      "\n",
      "         [[-4.4977e-02,  5.5156e-02,  8.6576e-03],\n",
      "          [ 2.0120e-02,  4.9497e-02, -2.1153e-02],\n",
      "          [-4.4801e-02,  2.5513e-02, -1.4547e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2833e-02, -4.2025e-02,  3.0753e-02],\n",
      "          [ 2.3451e-02,  5.2866e-02, -3.1602e-02],\n",
      "          [ 1.6113e-02, -5.6743e-02, -1.2637e-02]],\n",
      "\n",
      "         [[ 8.4211e-03,  5.6814e-02, -4.9991e-02],\n",
      "          [ 2.5576e-02, -2.5995e-02,  5.7408e-02],\n",
      "          [ 4.5542e-02, -1.1398e-02, -2.0678e-02]],\n",
      "\n",
      "         [[-3.8484e-02, -7.2270e-03, -2.6570e-02],\n",
      "          [-8.1156e-03,  5.3777e-02, -4.2098e-02],\n",
      "          [-3.2936e-02, -3.2815e-02,  6.3085e-03]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0544, -0.0178, -0.0376, -0.0580, -0.0241,  0.0332,  0.0052,  0.0420,\n",
      "         0.0264, -0.0109, -0.0372,  0.0178,  0.0046, -0.0350,  0.0178, -0.0026,\n",
      "        -0.0373, -0.0249,  0.0271,  0.0389,  0.0033, -0.0447, -0.0466, -0.0113,\n",
      "        -0.0396,  0.0493, -0.0136,  0.0212,  0.0499,  0.0045,  0.0225, -0.0076,\n",
      "        -0.0519, -0.0082,  0.0425,  0.0584,  0.0578, -0.0537,  0.0503, -0.0412,\n",
      "        -0.0172, -0.0392, -0.0352,  0.0069, -0.0265,  0.0128,  0.0153, -0.0506,\n",
      "        -0.0504,  0.0431, -0.0058,  0.0506, -0.0156,  0.0545,  0.0166, -0.0577,\n",
      "         0.0155, -0.0048,  0.0095, -0.0026, -0.0265, -0.0139,  0.0065, -0.0221],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0019, -0.0069, -0.0040,  ..., -0.0047, -0.0002,  0.0063],\n",
      "        [ 0.0006,  0.0038, -0.0068,  ..., -0.0053, -0.0034,  0.0072],\n",
      "        [-0.0043,  0.0103,  0.0093,  ..., -0.0064, -0.0024, -0.0066],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0094, -0.0057,  ..., -0.0013, -0.0095, -0.0070],\n",
      "        [-0.0007,  0.0085, -0.0007,  ..., -0.0073, -0.0063,  0.0063],\n",
      "        [-0.0081,  0.0021,  0.0074,  ...,  0.0078,  0.0053,  0.0058]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0100, -0.0085,  0.0086,  0.0037, -0.0071,  0.0035, -0.0003, -0.0102,\n",
      "        -0.0098,  0.0093,  0.0046, -0.0091, -0.0094,  0.0099,  0.0099, -0.0086,\n",
      "        -0.0064,  0.0071,  0.0002, -0.0029, -0.0045, -0.0094,  0.0098,  0.0013,\n",
      "         0.0087,  0.0021,  0.0073, -0.0053, -0.0082, -0.0070,  0.0057,  0.0101,\n",
      "         0.0027,  0.0102, -0.0044,  0.0019, -0.0030, -0.0012, -0.0094, -0.0085,\n",
      "        -0.0023, -0.0040, -0.0101,  0.0034, -0.0063, -0.0007,  0.0033, -0.0020,\n",
      "         0.0023, -0.0075,  0.0014,  0.0032, -0.0097,  0.0048, -0.0089,  0.0040,\n",
      "         0.0076,  0.0091,  0.0010,  0.0076, -0.0081,  0.0007,  0.0024,  0.0087,\n",
      "        -0.0003,  0.0102, -0.0069, -0.0031,  0.0022, -0.0056, -0.0035, -0.0057,\n",
      "        -0.0061,  0.0026,  0.0021, -0.0053,  0.0041,  0.0088,  0.0018, -0.0018,\n",
      "         0.0078, -0.0078,  0.0101, -0.0062,  0.0072, -0.0058,  0.0064,  0.0061,\n",
      "        -0.0044, -0.0045, -0.0054, -0.0055,  0.0046, -0.0067,  0.0083,  0.0033,\n",
      "        -0.0030,  0.0048,  0.0077, -0.0043,  0.0019,  0.0036, -0.0009, -0.0071,\n",
      "         0.0034, -0.0050, -0.0025, -0.0066, -0.0010,  0.0091, -0.0059,  0.0059,\n",
      "         0.0036, -0.0010, -0.0067,  0.0067, -0.0037,  0.0055,  0.0016, -0.0102,\n",
      "         0.0097, -0.0060,  0.0084,  0.0010, -0.0008,  0.0092,  0.0078,  0.0062],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0315, -0.0583, -0.0716,  ..., -0.0611, -0.0487, -0.0670],\n",
      "        [-0.0477,  0.0691, -0.0789,  ..., -0.0829, -0.0699, -0.0065],\n",
      "        [-0.0342,  0.0690, -0.0852,  ...,  0.0702,  0.0544,  0.0642],\n",
      "        ...,\n",
      "        [-0.0768,  0.0432,  0.0287,  ...,  0.0664, -0.0715,  0.0140],\n",
      "        [-0.0670,  0.0178,  0.0708,  ..., -0.0574,  0.0132,  0.0468],\n",
      "        [-0.0247, -0.0058,  0.0614,  ..., -0.0861, -0.0185, -0.0658]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0128, -0.0099,  0.0101, -0.0567, -0.0553,  0.0388,  0.0148, -0.0424,\n",
      "         0.0581, -0.0164], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# to check the params (tensors)\n",
    "\n",
    "model = Net()\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def initialize_training(gamma, learning_rate, local_dp):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # print(\"\\033[0;{};49m Rank {} is training on {}\".format(device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net().to(device)\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "\n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "                sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "                max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train_test(data, data2, device, model, optimizer, log_interval, local_dp, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        log_interval: The amount of rounds before logging intermediate loss.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "        data: dataset for train_loader will need to be specified here\n",
    "        data2: dataset for test_loader will need to be specified here\n",
    "    \"\"\"\n",
    "    # loading arguments/parameters from first RPC_method\n",
    "    \n",
    "    device, model, optimizer = initialize_training(gamma, learning_rate, local_dp)\n",
    "    \n",
    "    train_loader = data\n",
    "    \n",
    "    test_loader = data2\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        # Send the data and target to the device (cpu/gpu) the model is at\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Clear gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model on the data\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update model\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "    \n",
    "    # Adding differential privacy or not\n",
    "    if local_dp == True:\n",
    "        epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "#             print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are the parameters needed for the function\n",
    "Data loading and transforming (this will be done beforehand \n",
    "and then stored in './local/training.pt' and './testing.pt')\n",
    "\"\"\"\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "log_interval=10 \n",
    "\n",
    "gamma=0.7\n",
    "\n",
    "\n",
    "\n",
    "# train_data = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "#                                                           download=True,\n",
    "#                                                           train=True,\n",
    "#                                                           transform=transforms.Compose([\n",
    "#                                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "#                                                               transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "#                                                           ])), \n",
    "#                                            batch_size=64, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_data = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "#                                                           download=True,\n",
    "#                                                           train=False,\n",
    "#                                                           transform=transforms.Compose([\n",
    "#                                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "#                                                               transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "#                                                           ])), \n",
    "#                                            batch_size=1000, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# torch.save(train_data, \"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "# torch.save(test_data, \"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\testing.pt\")\n",
    "\n",
    "\n",
    "train_loader = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "\n",
    "\n",
    "test_loader = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\testing.pt\")\n",
    "\n",
    "\n",
    "local_dp = False\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "round = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, device, optimizer = initialize_training(gamma, learning_rate, local_dp)\n",
    "\n",
    "for epoch in range(1, epoch + 1):\n",
    "    torch.manual_seed(1)\n",
    "    RPC_train_test(train_loader, test_loader, model, device, optimizer, log_interval, local_dp, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epoch + 1):\n",
    "    train(train_loader, test_loader, 10, False, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FED AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def get_parameters(data, model):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.manual_seed(1)\n",
    "        for i in model.parameters():\n",
    "            # store parameters in dict\n",
    "            return {\"params\": parameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def average_parameters(data, model):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters(data, model) # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for param in parameters:\n",
    "            s = sum(parameters[i][1:])\n",
    "            average = s / len(parameters)\n",
    "            param.data = average\n",
    "            i = i + 1\n",
    "            return {\n",
    "                \"params_averaged\": parameters_averaged\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': Parameter containing:\n",
       " tensor([[[[ 0.1718, -0.1471, -0.0646],\n",
       "           [ 0.1565, -0.3138,  0.1999],\n",
       "           [-0.0686,  0.1696,  0.0463]]],\n",
       " \n",
       " \n",
       "         [[[-0.0408,  0.0925,  0.0164],\n",
       "           [ 0.1217, -0.1299, -0.0243],\n",
       "           [-0.0300,  0.0483, -0.0013]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2914,  0.1037, -0.1241],\n",
       "           [-0.2013, -0.0559, -0.1438],\n",
       "           [-0.1068,  0.0160,  0.1987]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1812, -0.3259,  0.2066],\n",
       "           [ 0.0931,  0.3162,  0.2200],\n",
       "           [-0.3037, -0.3169, -0.1608]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2927, -0.0555,  0.1427],\n",
       "           [-0.1549,  0.3271, -0.1410],\n",
       "           [ 0.2500,  0.0039, -0.1756]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1713, -0.1769,  0.0980],\n",
       "           [-0.0963, -0.0365, -0.3205],\n",
       "           [-0.1589,  0.1809, -0.0810]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3320,  0.2672, -0.0156],\n",
       "           [-0.2225,  0.2030,  0.1035],\n",
       "           [-0.2155,  0.2165,  0.2024]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2956, -0.1869, -0.0549],\n",
       "           [-0.0065,  0.0487, -0.2530],\n",
       "           [-0.2365,  0.1813, -0.0782]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1628,  0.0190,  0.1094],\n",
       "           [ 0.0733,  0.1212,  0.1652],\n",
       "           [-0.3087,  0.1678, -0.2344]]],\n",
       " \n",
       " \n",
       "         [[[-0.2515,  0.0203, -0.0568],\n",
       "           [ 0.1958, -0.1930, -0.2963],\n",
       "           [ 0.2426, -0.0494,  0.1875]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1072, -0.2500,  0.0670],\n",
       "           [ 0.0801, -0.2232, -0.1582],\n",
       "           [ 0.1137,  0.0597, -0.1418]]],\n",
       " \n",
       " \n",
       "         [[[-0.1009,  0.3053, -0.0617],\n",
       "           [ 0.1879,  0.1443, -0.2155],\n",
       "           [-0.2835,  0.3200,  0.0174]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2285,  0.0691,  0.1072],\n",
       "           [ 0.2490,  0.3161, -0.2212],\n",
       "           [ 0.0417,  0.2488,  0.2415]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2071, -0.2412, -0.2400],\n",
       "           [-0.2016,  0.0419,  0.3322],\n",
       "           [-0.2106,  0.1776, -0.1845]]],\n",
       " \n",
       " \n",
       "         [[[-0.3134, -0.0708,  0.1921],\n",
       "           [ 0.3095, -0.2070,  0.0723],\n",
       "           [ 0.2876,  0.2209,  0.2077]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2369,  0.2108,  0.0861],\n",
       "           [-0.2279, -0.2799, -0.1527],\n",
       "           [-0.0388, -0.2043,  0.1220]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1032, -0.0755,  0.1281],\n",
       "           [ 0.1077,  0.2035,  0.2245],\n",
       "           [-0.1129,  0.3257, -0.0385]]],\n",
       " \n",
       " \n",
       "         [[[-0.0115, -0.3146, -0.2145],\n",
       "           [-0.1947, -0.1426,  0.2370],\n",
       "           [-0.1089, -0.2491,  0.1282]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1067,  0.2159, -0.1725],\n",
       "           [ 0.0723, -0.1214, -0.0749],\n",
       "           [-0.2656, -0.1519, -0.1021]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1425,  0.0609,  0.0823],\n",
       "           [ 0.3327,  0.3249,  0.2273],\n",
       "           [ 0.0106, -0.2306,  0.2605]]],\n",
       " \n",
       " \n",
       "         [[[-0.0834, -0.0269, -0.2872],\n",
       "           [-0.0658, -0.2150,  0.3063],\n",
       "           [-0.2882, -0.2598, -0.0113]]],\n",
       " \n",
       " \n",
       "         [[[-0.1802,  0.1193, -0.1283],\n",
       "           [-0.1566,  0.0189,  0.2413],\n",
       "           [-0.2345,  0.1565,  0.2141]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3261, -0.2333,  0.0807],\n",
       "           [-0.2465,  0.2846, -0.1293],\n",
       "           [ 0.2008,  0.0099, -0.0260]]],\n",
       " \n",
       " \n",
       "         [[[-0.0106,  0.0567,  0.1571],\n",
       "           [ 0.0535,  0.1017, -0.2998],\n",
       "           [ 0.2428,  0.2906,  0.2755]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2464, -0.2405, -0.1236],\n",
       "           [ 0.2939, -0.2539,  0.3024],\n",
       "           [-0.2622, -0.2348,  0.1630]]],\n",
       " \n",
       " \n",
       "         [[[-0.2395, -0.0764,  0.2425],\n",
       "           [ 0.2640,  0.3153, -0.0676],\n",
       "           [-0.2591,  0.3282, -0.0710]]],\n",
       " \n",
       " \n",
       "         [[[-0.1371,  0.0813, -0.2331],\n",
       "           [ 0.2191,  0.2089, -0.2645],\n",
       "           [-0.2738, -0.0292,  0.1400]]],\n",
       " \n",
       " \n",
       "         [[[-0.0097, -0.1690,  0.0076],\n",
       "           [-0.3133, -0.2356, -0.2219],\n",
       "           [ 0.2745,  0.2938, -0.1132]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0150,  0.1487,  0.0399],\n",
       "           [-0.1669,  0.1922,  0.2049],\n",
       "           [-0.0193, -0.0410,  0.3029]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2914, -0.1890,  0.3261],\n",
       "           [ 0.0825, -0.2214,  0.1825],\n",
       "           [-0.2489,  0.3080, -0.2143]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0943,  0.1015,  0.0793],\n",
       "           [ 0.2765, -0.1384, -0.1407],\n",
       "           [-0.2889, -0.0137, -0.1579]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0134, -0.0683,  0.1106],\n",
       "           [ 0.2884,  0.0984, -0.1074],\n",
       "           [-0.1636, -0.2907,  0.2805]]]], requires_grad=True)}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(\"C:\\\\Users\\\\simon\\\\PycharmProjects\\\\torch-vantage6\\\\v6-ppsdg-py\\\\local\\\\MNIST\\\\processed\\\\training.pt\")\n",
    "model = Net()\n",
    "parameters=model.parameters()\n",
    "learning_rate=0.01\n",
    "log_interval=10 \n",
    "gamma=0.7\n",
    "round = 1\n",
    "epoch =1\n",
    "\n",
    "RPC_get_parameters(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-860d35ef0c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-148-60259dbd6231>\u001b[0m in \u001b[0;36maverage_parameters\u001b[1;34m(data, model)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0maverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "average_parameters(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, model, local_dp, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train and test with new parameters\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPC_fed_avg(data, local_dp, epoch, delta=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
