{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 2.6659e-01, -1.8754e-01,  1.4056e-01],\n",
      "          [-2.6799e-01,  2.4364e-02,  3.1870e-01],\n",
      "          [ 3.1865e-02, -1.6704e-01, -4.5359e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.2608e-01, -7.3469e-02, -1.9443e-01],\n",
      "          [-3.0724e-01,  2.8450e-01,  7.1407e-02],\n",
      "          [-1.9219e-01,  3.1421e-02, -2.4110e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.7427e-02, -3.0053e-01,  3.2676e-01],\n",
      "          [ 2.8281e-01,  4.3600e-02, -3.0157e-04],\n",
      "          [ 3.0820e-01, -1.8431e-01,  4.6132e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9954e-01,  2.8935e-02,  3.5751e-02],\n",
      "          [ 2.0908e-01, -3.1655e-01,  7.6181e-03],\n",
      "          [-3.0553e-01, -2.4606e-01,  2.1488e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.1892e-03, -6.4557e-02, -5.4479e-02],\n",
      "          [-3.0532e-01,  3.0861e-01, -1.8689e-01],\n",
      "          [-1.9416e-02,  5.9372e-02,  1.3005e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.1848e-02, -2.9730e-01, -7.9388e-02],\n",
      "          [ 2.2878e-01,  2.4139e-01, -1.5242e-01],\n",
      "          [ 4.6400e-02, -2.9634e-01, -2.5943e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1308e-01,  2.5997e-01,  2.3400e-01],\n",
      "          [-3.1195e-02,  6.4046e-02, -6.8897e-02],\n",
      "          [-2.7804e-01, -2.7229e-01,  1.5300e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.9368e-02,  2.1673e-01,  2.5529e-01],\n",
      "          [ 1.1116e-01,  1.5155e-01, -3.2382e-02],\n",
      "          [-3.6986e-02,  2.3154e-01,  2.1635e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.4533e-02, -1.8880e-01, -1.0923e-01],\n",
      "          [-1.9867e-01, -2.2007e-01, -8.8510e-03],\n",
      "          [ 2.9836e-02, -2.9946e-02, -2.4121e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9048e-01,  4.8581e-02, -1.5346e-01],\n",
      "          [-6.5892e-02,  2.0972e-01,  3.2500e-01],\n",
      "          [-2.7054e-01,  2.1247e-01,  1.2134e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0479e-01, -2.6689e-01,  2.4062e-01],\n",
      "          [ 1.3320e-01, -9.7947e-02, -2.5754e-01],\n",
      "          [ 2.2925e-01,  2.7348e-01,  1.8431e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.1589e-01, -2.3929e-01, -6.0228e-02],\n",
      "          [ 2.8557e-01, -2.5954e-01, -2.0348e-01],\n",
      "          [ 1.2001e-01, -2.0372e-01, -8.9574e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.4983e-02, -2.7306e-01,  3.1388e-01],\n",
      "          [-1.3390e-01,  2.6607e-01, -2.6074e-01],\n",
      "          [ 8.9566e-02, -3.2226e-01,  9.7736e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.7171e-01, -4.1258e-02,  1.8890e-02],\n",
      "          [ 1.9621e-01, -5.6052e-02,  1.0837e-01],\n",
      "          [-1.0869e-01, -3.1369e-02, -8.7229e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.0959e-02,  1.9638e-02,  1.6844e-01],\n",
      "          [ 2.3048e-01,  2.3438e-01,  9.5128e-03],\n",
      "          [-2.3586e-01,  1.4222e-01, -2.3448e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.5328e-03, -1.2916e-01,  1.4240e-01],\n",
      "          [-1.3606e-02,  1.8100e-01, -2.1896e-01],\n",
      "          [-2.6381e-02, -2.6015e-01,  4.6118e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0141e-01,  8.3054e-02,  1.4666e-01],\n",
      "          [ 3.1032e-01, -2.3275e-01,  2.6516e-01],\n",
      "          [-1.2418e-01,  1.6149e-01, -2.2751e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2031e-01, -3.1955e-01, -4.5665e-02],\n",
      "          [-9.1015e-02, -7.6994e-02, -4.1528e-02],\n",
      "          [-1.7023e-02, -2.3700e-01, -3.5332e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.3797e-01,  1.1383e-01, -3.0637e-01],\n",
      "          [-1.3028e-01,  1.4363e-01,  6.5252e-02],\n",
      "          [ 1.7364e-01, -2.0200e-02,  7.4176e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0336e-01, -1.9450e-01,  6.4667e-02],\n",
      "          [ 7.6724e-02,  2.8690e-01,  1.5367e-01],\n",
      "          [ 4.8714e-02,  2.8432e-01, -1.8837e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6851e-01, -1.5882e-01,  1.4669e-01],\n",
      "          [-1.4150e-01,  4.8925e-02, -1.0164e-01],\n",
      "          [ 2.1060e-01,  2.8630e-01, -7.4599e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.6995e-03,  7.6286e-03,  3.1897e-01],\n",
      "          [ 1.5607e-02, -6.8327e-02, -2.1324e-01],\n",
      "          [ 1.7885e-02, -9.5598e-02,  1.0755e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7018e-02,  3.9840e-02, -1.4616e-01],\n",
      "          [-1.4511e-01,  1.5197e-01, -2.0277e-01],\n",
      "          [-2.6571e-01,  2.4964e-01,  2.6352e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2313e-01,  3.2272e-01,  1.0530e-01],\n",
      "          [ 1.4677e-01, -1.4361e-01, -8.4072e-02],\n",
      "          [-8.2890e-02,  2.0046e-01, -2.3516e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.8743e-01, -1.6833e-01, -1.5147e-02],\n",
      "          [-2.7055e-01, -8.8183e-02,  1.4378e-01],\n",
      "          [ 9.3333e-02, -1.3928e-01, -3.1107e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.6945e-01, -1.4804e-01, -1.7197e-01],\n",
      "          [-9.6639e-02, -1.8991e-01, -3.0855e-01],\n",
      "          [ 2.5831e-01, -2.9163e-01,  2.5064e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.0906e-01,  2.5524e-01, -8.9304e-02],\n",
      "          [ 4.7871e-02, -3.0827e-01, -2.9874e-01],\n",
      "          [ 3.3153e-01, -2.5234e-01,  1.2565e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6187e-01,  4.4221e-02, -3.3167e-01],\n",
      "          [-2.5021e-01, -5.2447e-02,  1.0561e-01],\n",
      "          [ 9.4632e-03,  2.6199e-01,  2.7245e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4074e-01,  2.5058e-01, -2.8215e-01],\n",
      "          [-2.2702e-01, -2.4327e-01,  2.3448e-02],\n",
      "          [ 3.1919e-01, -1.9215e-01,  2.9901e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7813e-01, -1.1522e-01, -2.1046e-01],\n",
      "          [-5.5591e-02,  1.9752e-01, -1.7557e-02],\n",
      "          [-1.5872e-01, -1.1685e-01,  2.7447e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.0423e-01,  1.7635e-01,  1.4133e-01],\n",
      "          [-1.4054e-01,  1.3097e-01,  1.7800e-01],\n",
      "          [ 1.6692e-01,  1.5413e-01,  1.5993e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.3458e-01,  2.2050e-01, -2.3826e-01],\n",
      "          [ 1.7400e-01,  9.0479e-02, -4.7914e-02],\n",
      "          [ 1.9887e-01, -2.3804e-01,  2.1756e-01]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1877, -0.3168, -0.0731,  0.0028, -0.0868, -0.2395, -0.3097,  0.2892,\n",
      "         0.1890, -0.0396, -0.2345, -0.2473,  0.3073, -0.1756,  0.0134,  0.3266,\n",
      "        -0.2096, -0.1753,  0.1410,  0.1459,  0.1749, -0.1713, -0.0384, -0.2047,\n",
      "         0.2441, -0.1879,  0.1164, -0.0891, -0.2941, -0.0176, -0.0227, -0.0306],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0569, -0.0488, -0.0259],\n",
      "          [-0.0407,  0.0530,  0.0198],\n",
      "          [ 0.0439, -0.0324,  0.0305]],\n",
      "\n",
      "         [[ 0.0096,  0.0061,  0.0495],\n",
      "          [-0.0181,  0.0123, -0.0574],\n",
      "          [ 0.0239,  0.0344, -0.0123]],\n",
      "\n",
      "         [[ 0.0140,  0.0418,  0.0130],\n",
      "          [-0.0489,  0.0220, -0.0502],\n",
      "          [-0.0176, -0.0082, -0.0442]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0048, -0.0219,  0.0371],\n",
      "          [ 0.0169,  0.0481, -0.0250],\n",
      "          [ 0.0436,  0.0130,  0.0435]],\n",
      "\n",
      "         [[-0.0553,  0.0464, -0.0227],\n",
      "          [ 0.0361,  0.0139, -0.0192],\n",
      "          [ 0.0027,  0.0016, -0.0587]],\n",
      "\n",
      "         [[ 0.0410,  0.0134,  0.0155],\n",
      "          [ 0.0108, -0.0585,  0.0311],\n",
      "          [ 0.0066, -0.0552, -0.0063]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0544, -0.0196, -0.0345],\n",
      "          [ 0.0286, -0.0556,  0.0055],\n",
      "          [-0.0195,  0.0040, -0.0495]],\n",
      "\n",
      "         [[ 0.0340,  0.0147,  0.0170],\n",
      "          [-0.0099, -0.0212,  0.0166],\n",
      "          [-0.0060,  0.0421, -0.0490]],\n",
      "\n",
      "         [[ 0.0286,  0.0544,  0.0543],\n",
      "          [-0.0326,  0.0551,  0.0120],\n",
      "          [-0.0151,  0.0539,  0.0482]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0338,  0.0011,  0.0304],\n",
      "          [ 0.0531,  0.0169,  0.0043],\n",
      "          [-0.0242, -0.0375,  0.0167]],\n",
      "\n",
      "         [[-0.0566,  0.0529,  0.0003],\n",
      "          [-0.0157,  0.0312,  0.0131],\n",
      "          [-0.0358, -0.0354,  0.0484]],\n",
      "\n",
      "         [[ 0.0414, -0.0147,  0.0323],\n",
      "          [ 0.0128, -0.0022, -0.0073],\n",
      "          [ 0.0367, -0.0226, -0.0030]]],\n",
      "\n",
      "\n",
      "        [[[-0.0108,  0.0319,  0.0272],\n",
      "          [ 0.0553, -0.0091, -0.0551],\n",
      "          [-0.0192,  0.0354, -0.0331]],\n",
      "\n",
      "         [[-0.0282, -0.0306, -0.0574],\n",
      "          [-0.0023, -0.0368, -0.0380],\n",
      "          [ 0.0507,  0.0282, -0.0554]],\n",
      "\n",
      "         [[ 0.0279, -0.0448,  0.0393],\n",
      "          [ 0.0276, -0.0023, -0.0508],\n",
      "          [-0.0543,  0.0389,  0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0312,  0.0263,  0.0354],\n",
      "          [ 0.0414, -0.0456, -0.0230],\n",
      "          [ 0.0524,  0.0234, -0.0129]],\n",
      "\n",
      "         [[-0.0242, -0.0132, -0.0200],\n",
      "          [-0.0032, -0.0009, -0.0352],\n",
      "          [-0.0074,  0.0571, -0.0440]],\n",
      "\n",
      "         [[ 0.0436, -0.0261,  0.0229],\n",
      "          [-0.0161, -0.0380,  0.0050],\n",
      "          [-0.0543,  0.0459,  0.0331]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0221, -0.0251, -0.0469],\n",
      "          [ 0.0090, -0.0301, -0.0551],\n",
      "          [-0.0355, -0.0108, -0.0102]],\n",
      "\n",
      "         [[ 0.0146, -0.0583, -0.0282],\n",
      "          [ 0.0327,  0.0213, -0.0250],\n",
      "          [-0.0509,  0.0262,  0.0268]],\n",
      "\n",
      "         [[-0.0310,  0.0121, -0.0329],\n",
      "          [ 0.0464,  0.0470, -0.0432],\n",
      "          [ 0.0305,  0.0494, -0.0420]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0524, -0.0395,  0.0307],\n",
      "          [ 0.0166,  0.0136,  0.0290],\n",
      "          [ 0.0139, -0.0296,  0.0118]],\n",
      "\n",
      "         [[-0.0289, -0.0432, -0.0252],\n",
      "          [ 0.0289, -0.0431, -0.0511],\n",
      "          [-0.0068, -0.0335,  0.0210]],\n",
      "\n",
      "         [[ 0.0005,  0.0085,  0.0081],\n",
      "          [-0.0540, -0.0518, -0.0497],\n",
      "          [ 0.0431, -0.0272, -0.0389]]],\n",
      "\n",
      "\n",
      "        [[[-0.0179, -0.0397,  0.0092],\n",
      "          [ 0.0181, -0.0080, -0.0282],\n",
      "          [-0.0485,  0.0194, -0.0589]],\n",
      "\n",
      "         [[-0.0129,  0.0462, -0.0238],\n",
      "          [-0.0375,  0.0015, -0.0460],\n",
      "          [-0.0420,  0.0068, -0.0571]],\n",
      "\n",
      "         [[-0.0293,  0.0490,  0.0473],\n",
      "          [ 0.0382, -0.0477,  0.0001],\n",
      "          [-0.0112,  0.0083, -0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002,  0.0107, -0.0394],\n",
      "          [ 0.0261,  0.0516,  0.0457],\n",
      "          [-0.0517,  0.0066,  0.0056]],\n",
      "\n",
      "         [[-0.0577,  0.0488, -0.0251],\n",
      "          [-0.0152,  0.0340,  0.0501],\n",
      "          [-0.0061,  0.0218,  0.0332]],\n",
      "\n",
      "         [[ 0.0400, -0.0050,  0.0260],\n",
      "          [ 0.0301, -0.0554, -0.0217],\n",
      "          [ 0.0198,  0.0396,  0.0550]]],\n",
      "\n",
      "\n",
      "        [[[-0.0432,  0.0162,  0.0282],\n",
      "          [-0.0156,  0.0489,  0.0046],\n",
      "          [-0.0430, -0.0548,  0.0583]],\n",
      "\n",
      "         [[-0.0306,  0.0112,  0.0208],\n",
      "          [-0.0125,  0.0108, -0.0372],\n",
      "          [ 0.0066, -0.0243,  0.0206]],\n",
      "\n",
      "         [[ 0.0411,  0.0586,  0.0154],\n",
      "          [ 0.0441, -0.0117,  0.0486],\n",
      "          [-0.0391, -0.0311, -0.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0499, -0.0299,  0.0338],\n",
      "          [-0.0444,  0.0148, -0.0105],\n",
      "          [-0.0048, -0.0198,  0.0523]],\n",
      "\n",
      "         [[-0.0409,  0.0059,  0.0476],\n",
      "          [-0.0395, -0.0503,  0.0282],\n",
      "          [-0.0418,  0.0418, -0.0311]],\n",
      "\n",
      "         [[-0.0311, -0.0340,  0.0542],\n",
      "          [-0.0468, -0.0044, -0.0423],\n",
      "          [ 0.0056,  0.0255, -0.0345]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0457, -0.0480, -0.0249,  0.0188, -0.0367,  0.0445, -0.0373,  0.0065,\n",
      "        -0.0387, -0.0190,  0.0435,  0.0147,  0.0074, -0.0238,  0.0486, -0.0435,\n",
      "        -0.0328,  0.0479,  0.0337, -0.0485,  0.0570,  0.0587, -0.0038, -0.0565,\n",
      "        -0.0039,  0.0258, -0.0117, -0.0439, -0.0082, -0.0446, -0.0370,  0.0323,\n",
      "         0.0251, -0.0276, -0.0550, -0.0236, -0.0518, -0.0381, -0.0126, -0.0578,\n",
      "        -0.0069, -0.0322, -0.0022, -0.0352, -0.0343,  0.0181,  0.0437,  0.0477,\n",
      "         0.0436, -0.0158, -0.0172,  0.0209, -0.0049, -0.0337, -0.0049, -0.0499,\n",
      "         0.0561,  0.0137, -0.0162,  0.0508, -0.0494,  0.0474, -0.0476, -0.0058],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0002,  0.0082, -0.0069,  ..., -0.0048, -0.0046,  0.0085],\n",
      "        [-0.0050,  0.0067,  0.0051,  ..., -0.0042, -0.0077, -0.0053],\n",
      "        [ 0.0098,  0.0007, -0.0087,  ...,  0.0044,  0.0081, -0.0050],\n",
      "        ...,\n",
      "        [ 0.0043,  0.0068, -0.0077,  ..., -0.0053, -0.0065,  0.0021],\n",
      "        [ 0.0022,  0.0078,  0.0078,  ...,  0.0004, -0.0008, -0.0019],\n",
      "        [ 0.0014,  0.0011, -0.0026,  ..., -0.0066, -0.0084,  0.0086]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0041,  0.0086,  0.0032,  0.0009, -0.0084, -0.0059, -0.0012, -0.0043,\n",
      "         0.0005, -0.0097, -0.0053, -0.0059, -0.0048, -0.0024, -0.0050,  0.0027,\n",
      "         0.0008,  0.0051,  0.0065, -0.0071, -0.0089, -0.0015,  0.0057,  0.0016,\n",
      "         0.0082, -0.0018, -0.0028,  0.0032, -0.0092, -0.0026, -0.0050,  0.0057,\n",
      "        -0.0054, -0.0062, -0.0031,  0.0088, -0.0003, -0.0016,  0.0078, -0.0070,\n",
      "         0.0087, -0.0090,  0.0090, -0.0092, -0.0071,  0.0075,  0.0018,  0.0103,\n",
      "         0.0067, -0.0095,  0.0053, -0.0018, -0.0092, -0.0021,  0.0099,  0.0046,\n",
      "        -0.0053, -0.0004, -0.0015, -0.0025, -0.0023, -0.0034,  0.0002,  0.0010,\n",
      "        -0.0102,  0.0028,  0.0041, -0.0084, -0.0042, -0.0036, -0.0039,  0.0097,\n",
      "         0.0088, -0.0005, -0.0031,  0.0023,  0.0034,  0.0095, -0.0002,  0.0034,\n",
      "        -0.0009,  0.0030, -0.0067,  0.0089,  0.0059,  0.0021,  0.0028,  0.0064,\n",
      "         0.0070, -0.0026,  0.0007,  0.0014, -0.0021, -0.0032, -0.0046,  0.0015,\n",
      "         0.0081, -0.0008,  0.0072, -0.0066,  0.0083, -0.0090, -0.0049, -0.0077,\n",
      "         0.0101,  0.0032,  0.0030,  0.0037,  0.0092,  0.0065,  0.0075, -0.0052,\n",
      "         0.0102,  0.0070, -0.0065, -0.0094,  0.0018,  0.0027, -0.0080,  0.0042,\n",
      "         0.0085, -0.0054, -0.0003, -0.0004, -0.0073,  0.0041,  0.0019,  0.0038],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0132,  0.0484, -0.0272,  ..., -0.0835,  0.0399, -0.0307],\n",
      "        [-0.0687,  0.0454, -0.0848,  ..., -0.0498,  0.0613, -0.0014],\n",
      "        [-0.0495,  0.0347,  0.0854,  ...,  0.0135,  0.0435,  0.0237],\n",
      "        ...,\n",
      "        [-0.0592,  0.0373, -0.0570,  ..., -0.0663,  0.0541, -0.0627],\n",
      "        [ 0.0051,  0.0093,  0.0214,  ...,  0.0718,  0.0055, -0.0150],\n",
      "        [ 0.0812,  0.0587, -0.0372,  ...,  0.0833,  0.0364,  0.0158]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0563,  0.0758, -0.0054,  0.0233, -0.0614, -0.0379,  0.0160,  0.0416,\n",
      "        -0.0170, -0.0571], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-2ec2dbcec062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# The first entry of the provided parameters when using dist.gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# method also contains the value from the server, remove that one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mminus_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Calculate the average by summing and dividing by the number of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "parameters = model.parameters()\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        # The first entry of the provided parameters when using dist.gather\n",
    "        # method also contains the value from the server, remove that one\n",
    "        minus_server = parameters[i][1:]\n",
    "        # Calculate the average by summing and dividing by the number of\n",
    "        # workers\n",
    "        s = sum(minus_server)\n",
    "        average = s/len(minus_server)\n",
    "        # Change the parameter of the global model to the average\n",
    "        param.data = average\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def RPC_initialize_training(data, gamma, learning_rate, local_dp):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # print(\"\\033[0;{};49m Rank {} is training on {}\".format(device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "                sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "                max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train(data, log_interval, local_dp, epoch, round, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        log_interval: The amount of rounds before logging intermediate loss.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "    \"\"\"\n",
    "    # loading arguments/parameters from first RPC_method\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp) # is this allowed in vantage6? calling one RPC_method in another?\n",
    "            \n",
    "    model.train()\n",
    "# , (data, target)\n",
    "    for batch_idx, data in enumerate(data,0): \n",
    "        \n",
    "#         batch = (data, target)\n",
    "        data, target = data\n",
    "        # Send the data and target to the device (cpu/gpu) the model is at\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        # Clear gradient buffers\n",
    "        optimizer.zero_grad()\n",
    "        # Run the model on the data\n",
    "        output = model(data)\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(loss)\n",
    "#     # Update the model weights\n",
    "#     if train:\n",
    "#         optimizer.step()\n",
    "#     return loss\n",
    "\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print('\\033[0;{};49m Train on Rank {}, Round {}, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#             round, epoch, batch_idx * len(batch[0]), len(train_loader.dataset),\n",
    "#             100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "\n",
    "    # Adding differential privacy or not\n",
    "    if local_dp == True:  \n",
    "        epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            # print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))\n",
    "     \n",
    "    \n",
    "    \n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "# # This function trains the neural network for one epoch\n",
    "# def train(epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         # Move the input and target data on the GPU\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # Zero out gradients from previous step\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass of the neural net\n",
    "#         output = model(data)\n",
    "#         # Calculation of the loss function\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         # Backward pass (gradient computation)\n",
    "#         loss.backward()\n",
    "#         # Adjusting the parameters according to the loss function\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def RPC_test(data):\n",
    "    \"\"\"\n",
    "    Tests the model.\n",
    "\n",
    "    Args:\n",
    "        color: The color for the terminal output for this worker.\n",
    "        model: The model to test.\n",
    "        device: The device to test the model on.\n",
    "        test_loader: The local loader for test local. -> no inside function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#     test_loader = torch.load(\"./testing.pt\")\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                              train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(test_loss)\n",
    "\n",
    "    # print('\\033[0;{};49m \\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #         test_loss, correct, len(test_loader.dataset),\n",
    "    #         100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def RPC_get_parameters(data, model, parameters):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    data_size = len(data) // 3 # number of nodes# size of dataset\n",
    "    \n",
    "    weights = []\n",
    "    # Gather the data sizes on the server\n",
    "    tensor_weights = torch.tensor(data_size)\n",
    "    tensor_weights = tensor_weights[1:]\n",
    "    # Convert all tensors back to weights\n",
    "    for tensor in tensor_weights:\n",
    "            weights.append(tensor.item())\n",
    "\n",
    "            \n",
    "    for parameters in model.parameters():\n",
    "        return {\n",
    "            \"params\": parameters,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this might need to be combined with training, so that train \n",
    "returns the parameters or that it at least calls the results of training function\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def average_parameters(data, model):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters() # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for parameters in model.parameters():\n",
    "            average = sum(x * y for x, y in zip(parameters[i], weights)) / sum(weights)\n",
    "            parameters.data = average\n",
    "            i = i + 1\n",
    "        return {\n",
    "            \"params_averaged\": model\n",
    "        }\n",
    "    \n",
    "\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#     # The first entry of the provided parameters when using dist.gather\n",
    "#     # method also contains the value from the server, remove that one\n",
    "#     minus_server = parameters[i][1:]\n",
    "#     # Calculate the average by summing and dividing by the number of\n",
    "#     # workers\n",
    "#     s = sum(minus_server)\n",
    "#     average = s/len(minus_server)\n",
    "#     # Change the parameter of the global model to the average\n",
    "#     param.data = average\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, local_dp, model, device, optimizer, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "    model = RPC_average_parameters()\n",
    "    \n",
    "    for epoch in range(1, epoch + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train(data, local_dp, model, device, optimizer, epoch, delta=1e-5)\n",
    "        # Test the model on the workers\n",
    "        RPC_test(data, model, device)\n",
    "\n",
    "    gather_params = model.get_parameters() # or model.parameters()\n",
    "\n",
    "    RPC_train(model.RPC_average_parameters_weighted(gather_params))\n",
    "\n",
    "    return model, parameters\n",
    "\n",
    "\n",
    "    ## OR \n",
    "\n",
    "#     parameters = RPC_average_parameters_weighted(data, model, parameters, weights) # then uses those parameters for training\n",
    "\n",
    "\n",
    "\n",
    "        # # Gather the parameters after the training round on the server\n",
    "        #     gather_params = coor.gather_parameters(rank, model, group_size + 1, subgroup)\n",
    "        #\n",
    "        #     # If the server\n",
    "        #     if rank == 0:\n",
    "        #         # Calculate the average of the parameters and adjust global model\n",
    "        #         coor.average_parameters_weighted(model, gather_params, weights)\n",
    "        #\n",
    "        #     # Send the new model parameters to the workers\n",
    "        #     coor.broadcast_parameters(model, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTODO, the RPC_initialize_training method is \\ncalled in the RPC_train method, \\nand yet it doesn't know where to get model, device, optimizer from. \\n\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "These are the parameters needed for the function\n",
    "Data loading and transforming (this will be done beforehand \n",
    "and then stored in './local/training.pt' and './testing.pt')\n",
    "\"\"\"\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "gamma=0.7\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "#                                                           download=True, \n",
    "#                                                           transform=transforms.Compose([\n",
    "#                                                               transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "#                                                               transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "#                                                           ])), \n",
    "#                                            batch_size=10, \n",
    "#                                            shuffle=True)\n",
    "data = pd.read_csv(\"/Users/simontokloth/PycharmProjects/torch-vantage6/v6-ppsdg-py/local/mnist_train.csv\")\n",
    "data = data_loader\n",
    "\n",
    "\n",
    "local_dp = True\n",
    "\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "round = 1\n",
    "\n",
    "\"\"\"\n",
    "TODO, the RPC_initialize_training method is \n",
    "called in the RPC_train method, \n",
    "and yet it doesn't know where to get model, device, optimizer from. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:518: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  \"The sample rate will be defined from ``batch_size`` and ``sample_size``.\"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n",
      "/Users/simontokloth/anaconda3/envs/ppsdg/lib/python3.7/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2792, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3574, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3095, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3172, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3023, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2945, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2522, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3519, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2895, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2183, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2276, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2940, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2814, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2439, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2546, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2976, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1888, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3296, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2411, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2261, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3369, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2615, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3089, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3935, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3451, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1135, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3170, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2346, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2052, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4743, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4063, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3771, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2428, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2745, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3398, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1635, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4530, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3796, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2933, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3049, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0596, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2717, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2636, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9815, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2752, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0644, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3351, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4535, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2079, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3242, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2811, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1383, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2837, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3560, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2614, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3162, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0412, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1192, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2003, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1280, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2026, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2829, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4449, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2026, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1565, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0548, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1151, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0265, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3631, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0403, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4613, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1977, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9404, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5377, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9944, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2604, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2785, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4169, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9841, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3170, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9284, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0646, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9464, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1605, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2025, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0818, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0942, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1641, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9707, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1054, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0507, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1858, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6569, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9827, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5967, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3055, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1039, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4007, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2500, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2406, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0267, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1408, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5596, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1472, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0101, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4471, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1055, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2477, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6287, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0084, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3418, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3664, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4102, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1275, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9967, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2309, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2016, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3077, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0320, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0467, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2355, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1774, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1992, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9625, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5891, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2586, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4559, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5017, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5133, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1893, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2727, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1773, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5956, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5043, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0108, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4573, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0056, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4223, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9552, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4536, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1713, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6887, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0799, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1827, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8628, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9209, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5660, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2934, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8194, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4959, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5487, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1093, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3058, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5841, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1414, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1685, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8113, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4683, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1780, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6945, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1982, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5112, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1838, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7428, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0405, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2196, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9699, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8855, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9419, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0078, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4548, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4399, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4427, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9727, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1805, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9024, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2461, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2069, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7909, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0580, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9838, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9898, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8323, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3516, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7394, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3420, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7981, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3496, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2976, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5287, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2336, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2572, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2447, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0018, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7338, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2756, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6204, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3701, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4491, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6924, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6930, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5315, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1798, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3100, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5051, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5204, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4790, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8334, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2960, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0924, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9815, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1073, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5139, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9144, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3274, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7865, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7892, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3578, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2617, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9235, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0318, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9261, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0087, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4461, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8920, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6767, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5562, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4193, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0119, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3557, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5456, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0565, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1310, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3422, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8690, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8461, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9128, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2951, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9403, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5529, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4359, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7915, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2805, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0219, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3252, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7281, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0066, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4292, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0304, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1147, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8261, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1987, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5078, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7192, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8438, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6955, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2664, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3366, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3612, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9631, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4627, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1704, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8555, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9016, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6285, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1213, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1649, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9420, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3445, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4927, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4319, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2565, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7464, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4361, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6054, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4888, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7905, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2422, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0897, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2478, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9002, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7706, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6734, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4818, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1246, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9846, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9764, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7337, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6634, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2452, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7829, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7584, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3949, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4354, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0026, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8574, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6489, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1927, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8988, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0872, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2647, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8534, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7967, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8917, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2769, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8803, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8656, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5908, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0911, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8328, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1833, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3331, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3217, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8019, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0618, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8936, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7970, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3616, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7346, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3770, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9086, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5886, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6507, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9893, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1844, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0059, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0783, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0057, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2840, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4929, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3385, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7056, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8509, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4290, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0214, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4114, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1908, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3473, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6129, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8599, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6455, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5010, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7065, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8932, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7635, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8781, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9917, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3845, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2366, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2508, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9612, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0428, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7476, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5566, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0940, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3076, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2105, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4372, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8688, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0336, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7107, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7389, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6727, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1040, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5775, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4453, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0803, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4981, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1021, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1780, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1410, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9421, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5517, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6788, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4294, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1658, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8075, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0968, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9599, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9181, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6020, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6395, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4726, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4479, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9493, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6623, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1857, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5026, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7021, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9951, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0750, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5774, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0389, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2669, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1963, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5257, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2628, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6074, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7638, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7077, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5327, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9482, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7040, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4828, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9606, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4064, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0056, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7129, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1173, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2807, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6758, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7692, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9266, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4081, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9990, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5322, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9943, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0778, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5033, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5843, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8973, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9530, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2700, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2698, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6775, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2072, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8573, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0999, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1598, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4414, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1342, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7260, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7954, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2161, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7634, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6423, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3325, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2396, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5195, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1111, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6472, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8449, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0878, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6920, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1716, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1582, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1949, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8652, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3607, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7835, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8194, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1549, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0208, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2880, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2585, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1363, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6975, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5436, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4312, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5834, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9897, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9498, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0006, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8918, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5746, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7433, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0800, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7088, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7612, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6546, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7905, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6941, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3451, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1314, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8953, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6878, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5326, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5155, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7490, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8778, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5442, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6831, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7939, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1714, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1881, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2184, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7895, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1272, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2612, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2354, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3770, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2361, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9252, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4429, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6093, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2292, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3680, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1522, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6461, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4448, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5732, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5642, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4912, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3317, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4322, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3497, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1465, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1479, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3806, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1012, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3964, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6109, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3898, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0306, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5889, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0914, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9602, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5323, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8661, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3349, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3289, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9323, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6495, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4133, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6548, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8510, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7402, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9667, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1708, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9157, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9895, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5041, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6553, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0641, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1813, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1183, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2195, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8953, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4531, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9863, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6711, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4672, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9800, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3882, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4440, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5108, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6573, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0743, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2903, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3273, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0200, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3211, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6616, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9326, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6930, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5804, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4920, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7374, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8536, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0987, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9867, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8998, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2678, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4086, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1489, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3171, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8870, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9284, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2006, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5758, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5734, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0032, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3972, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5194, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9693, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3804, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3848, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8345, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8324, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1018, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8906, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4558, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7404, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8419, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5295, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5646, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1757, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1621, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4467, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0937, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6682, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9368, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4109, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9129, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8552, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2645, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6131, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0910, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7756, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1101, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0454, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6907, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1551, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7054, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2444, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2752, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7610, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8098, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8952, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4570, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3687, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3539, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8763, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2539, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9366, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4247, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5626, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0044, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3814, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8177, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4928, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6365, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6205, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8668, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6546, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5082, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1108, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7758, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9840, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5211, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3009, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7295, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1393, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7824, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2341, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9934, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9999, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8004, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4632, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3618, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5007, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5769, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8791, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6596, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1360, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1854, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9858, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1643, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1042, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0248, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9452, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6674, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3360, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7950, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4233, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4234, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4413, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8873, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7450, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5150, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7396, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8507, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9712, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4677, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2984, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0409, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7306, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3652, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3864, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2159, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1938, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9973, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7193, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6317, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1697, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0122, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8418, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9696, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6503, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3781, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5243, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0923, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7058, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3582, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8694, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3174, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6484, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3727, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6788, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6392, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7192, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5085, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0670, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2093, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7765, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7729, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0404, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1673, grad_fn=<NllLossBackward>)\n",
      "tensor(5.9951, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8546, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8364, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4674, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9464, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7839, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3564, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9914, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6251, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2986, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0699, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0899, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2804, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6774, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5674, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3762, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5157, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7601, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8219, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5217, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3115, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1063, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7748, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2273, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1642, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7292, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0960, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8764, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5649, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3066, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6535, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8043, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8863, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6574, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7289, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6660, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4193, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1710, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5171, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0524, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4822, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5068, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2219, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2002, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1659, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5540, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7876, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2181, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6009, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9418, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5511, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0802, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4349, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8773, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3467, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7116, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9081, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2331, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7540, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5962, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2947, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4250, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1705, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6612, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9268, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2110, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6337, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9661, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3287, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8887, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2884, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3037, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3674, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8158, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1581, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4635, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9607, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0572, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8335, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4396, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9905, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2768, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8426, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2964, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1688, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2137, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7830, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9328, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3001, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2685, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0514, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8303, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6642, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6785, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0060, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7289, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3731, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9359, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8694, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4279, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7958, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8885, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7863, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0176, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9001, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5905, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8946, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9810, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8335, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9973, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1836, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1210, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0075, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0858, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8479, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0375, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1173, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1628, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7129, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8874, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4345, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2542, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3683, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2238, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4140, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4676, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0753, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0791, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1381, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0972, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2012, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0606, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0775, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8637, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3257, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2737, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0461, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6323, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5862, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8619, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4294, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1903, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8994, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9832, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9219, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3234, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8679, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7876, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2859, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8934, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7273, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2502, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1452, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4560, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7072, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7234, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0014, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7095, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9889, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2581, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0277, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9252, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4905, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2149, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7912, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5216, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1568, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8776, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0287, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5789, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8418, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3220, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8974, grad_fn=<NllLossBackward>)\n",
      "tensor(6.4446, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7732, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9545, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0656, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5260, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5489, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1560, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0438, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1559, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1421, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7838, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6813, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7272, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6775, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9622, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9925, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1715, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0783, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3381, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4448, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4790, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7852, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3612, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0529, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9965, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2371, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3351, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4970, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2901, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6786, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9422, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5667, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2203, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1459, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6816, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5758, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2745, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8785, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9347, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5090, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6284, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4459, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6421, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0965, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6594, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0856, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0344, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2332, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0745, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1245, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2171, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5932, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9888, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4329, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7847, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6075, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3026, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4949, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9463, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7663, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3397, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2952, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4805, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9571, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7277, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4256, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6042, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1867, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0315, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1220, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7706, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2072, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2437, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6377, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2655, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8188, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8821, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3303, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6848, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7876, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8300, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2192, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7722, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5538, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6719, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2459, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1631, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5049, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8510, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3919, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3674, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4469, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7421, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7748, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8359, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6820, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1182, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4198, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3037, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8833, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3789, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2134, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5472, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9555, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6567, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2599, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7258, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8792, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9515, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1899, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9436, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7486, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6155, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3709, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0221, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4583, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9016, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8192, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9227, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1165, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5129, grad_fn=<NllLossBackward>)\n",
      "tensor(6.6077, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1192, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1651, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7474, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3443, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7237, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4400, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2575, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8961, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0035, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1209, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2450, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1393, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8505, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2188, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2212, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8981, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9668, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9045, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2802, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5571, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9743, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4293, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6650, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0989, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5222, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2890, grad_fn=<NllLossBackward>)\n",
      "tensor(5.8704, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4262, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5734, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5669, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3992, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1327, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1211, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7655, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3349, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9390, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0981, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9589, grad_fn=<NllLossBackward>)\n",
      "tensor(6.3881, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1077, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4404, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3470, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0134, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4132, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0378, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5482, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8170, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8388, grad_fn=<NllLossBackward>)\n",
      "tensor(6.6099, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3801, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3452, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2001, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8059, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6572, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5858, grad_fn=<NllLossBackward>)\n",
      "tensor(5.7142, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6027, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4769, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7828, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2659, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7540, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5555, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1460, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1454, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0951, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6483, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1707, grad_fn=<NllLossBackward>)\n",
      "tensor(6.7599, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8457, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8359, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0969, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3669, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6866, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8978, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9185, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1817, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0187, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0314, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3332, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2405, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2270, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4873, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1144, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0555, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8568, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4596, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0651, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6836, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4573, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6519, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1748, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7101, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2434, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8659, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4434, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3028, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5815, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2856, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4216, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0374, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8406, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5429, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0754, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7101, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9666, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3128, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1482, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5767, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2922, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2282, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1793, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8916, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4906, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0130, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5650, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8788, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7745, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4166, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7506, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5614, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0502, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7060, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7869, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7464, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2093, grad_fn=<NllLossBackward>)\n",
      "tensor(3.5335, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4298, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7057, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6880, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1471, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1924, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5976, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9220, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2446, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9608, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8019, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8943, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5518, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9594, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1378, grad_fn=<NllLossBackward>)\n",
      "tensor(3.7650, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3489, grad_fn=<NllLossBackward>)\n",
      "tensor(3.1707, grad_fn=<NllLossBackward>)\n",
      "tensor(2.8857, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9398, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7118, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8583, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4757, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3878, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3463, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5313, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3912, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2092, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3279, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3678, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5693, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8990, grad_fn=<NllLossBackward>)\n",
      "tensor(7.2491, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7299, grad_fn=<NllLossBackward>)\n",
      "tensor(3.8705, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0509, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0670, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6739, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3420, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3764, grad_fn=<NllLossBackward>)\n",
      "tensor(2.7198, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2546, grad_fn=<NllLossBackward>)\n",
      "tensor(6.1244, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8727, grad_fn=<NllLossBackward>)\n",
      "tensor(4.1101, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7575, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4341, grad_fn=<NllLossBackward>)\n",
      "tensor(2.5794, grad_fn=<NllLossBackward>)\n",
      "tensor(4.0694, grad_fn=<NllLossBackward>)\n",
      "tensor(3.6645, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4746, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3510, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7276, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2122, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# local_dp = False\n",
    "# RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "log_interval = 5\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# model = Net()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "RPC_train(data, log_interval, local_dp, epoch, round)\n",
    "\n",
    "# RPC_test(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
