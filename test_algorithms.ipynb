{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from opacus import PrivacyEngine\n",
    "from vantage6.tools.util import info, warn\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# initialises training\n",
    "\n",
    "def RPC_initialize_training(data, gamma, learning_rate, local_dp):\n",
    "    \"\"\"\n",
    "    Initializes the model, optimizer and scheduler and shares the parameters\n",
    "    with all the workers in the group.\n",
    "\n",
    "    This should be sent from server to all nodes.\n",
    "\n",
    "    Args:\n",
    "        data: contains the local data from the node\n",
    "        gamma: Learning rate step gamma (default: 0.7)\n",
    "        learning_rate: The learning rate for training.\n",
    "        cuda: Should we use CUDA?\n",
    "        local_dp: bool whether to apply local_dp or not.\n",
    "\n",
    "    Returns:\n",
    "        Returns the device, model, optimizer and scheduler.\n",
    "    \"\"\"\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.dropout1 = nn.Dropout(0.25)\n",
    "            self.dropout2 = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(9216, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            output = F.log_softmax(x, dim=1)\n",
    "            return output\n",
    "    \n",
    "    # Determine the device to train on\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # print(\"\\033[0;{};49m Rank {} is training on {}\".format(device))\n",
    "\n",
    "    # Initialize model and send parameters of server to all workers\n",
    "    model = Net()\n",
    "    model.to(device)\n",
    "\n",
    "    # intializing optimizer and scheduler\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # adding DP if true\n",
    "    if local_dp == True:\n",
    "        privacy_engine = PrivacyEngine(model, batch_size=64,\n",
    "            sample_size=60000, alphas=range(2,32), noise_multiplier=1.3,\n",
    "            max_grad_norm=1.0,)\n",
    "        privacy_engine.attach(optimizer)\n",
    "\n",
    "    # returns device, model, optimizer which will be needed in train and test\n",
    "    return device, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# basic training of the model\n",
    "\n",
    "# Question: train gets model, device, optimizer from initialize_training, which is specified within train function, \n",
    "# why do I need to call it again before executing the function? Because in vantage6 when I sent the tasks I cannot define that but only in the master function\n",
    "\n",
    "\n",
    "def RPC_train(data, local_dp, epoch, round, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training the model on all batches.\n",
    "    Args:\n",
    "        epoch: The number of the epoch the training is in.\n",
    "        round: The number of the round the training is in.\n",
    "        local_dp: Training with local DP?\n",
    "        delta: The delta value of DP to aim for (default: 1e-5).\n",
    "    \"\"\"\n",
    "    # loading arguments/parameters from first RPC_method\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp) # is this allowed in vantage6? calling one RPC_method in another?\n",
    "\n",
    "#     train_loader = torch.load(\"./training.pt\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                            train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "    \n",
    "    for round in range(1, round + 1):\n",
    "        \n",
    "        for epoch in range(1, epoch + 1):\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                # Send the local and target to the device (cpu/gpu) the model is at (either send to the cpu or to the gpu, but the local is already on the worker node); model.send(local.location)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # define batch\n",
    "                batch = (data, target)\n",
    "                # Clear gradient buffers\n",
    "                optimizer.zero_grad()\n",
    "                # Run the model on the local\n",
    "                output = model(data)\n",
    "                # Calculate the loss\n",
    "                loss = F.nll_loss(output, target)\n",
    "                # Calculate the gradients\n",
    "                loss.backward()\n",
    "                # Update the model weights \n",
    "                optimizer.step()\n",
    "                return loss\n",
    "\n",
    "            print(loss.item())\n",
    "\n",
    "                # Logging needed if want the same output as torch.dist\n",
    "            #     print('\\033[0;{};49m Train on Rank {}, Round {}, Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #         round, epoch, batch_idx * len(batch[0]), len(train_loader.dataset),\n",
    "            #         100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            # Adding differential privacy or not\n",
    "            if local_dp == True:  \n",
    "                epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "                # print(\"\\033[0;{};49m Epsilon {}, best alpha {}\".format(epsilon, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "def RPC_test(data):\n",
    "    \"\"\"\n",
    "    Tests the model.\n",
    "\n",
    "    Args:\n",
    "        color: The color for the terminal output for this worker.\n",
    "        model: The model to test.\n",
    "        device: The device to test the model on.\n",
    "        test_loader: The local loader for test local. -> no inside function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "#     test_loader = torch.load(\"./testing.pt\")\n",
    "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                              train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "    device, model, optimizer = RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send the local and target to the device (cpu/gpu) the model is at\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Run the model on the local\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # Check whether prediction was correct\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(test_loss)\n",
    "\n",
    "    # print('\\033[0;{};49m \\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #         test_loss, correct, len(test_loader.dataset),\n",
    "    #         100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis might need to be combined with training, so that train \\nreturns the parameters or that it at least calls the results of training function\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FedAvg gathering of parameters \n",
    "\n",
    "def RPC_get_parameters(data, model, parameters, weights):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes\n",
    "    \"\"\"\n",
    "    data_size = len(data) // 3 # number of nodes# size of dataset\n",
    "    \n",
    "    weights = []\n",
    "    # Gather the data sizes on the server\n",
    "    tensor_weights = torch.tensor(data_size)\n",
    "    tensor_weights = tensor_weights[1:]\n",
    "    # Convert all tensors back to weights\n",
    "    for tensor in tensor_weights:\n",
    "            weights.append(tensor.item())\n",
    "\n",
    "            \n",
    "    for parameters in model.parameters():\n",
    "        return {\n",
    "            \"params\": parameters,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this might need to be combined with training, so that train \n",
    "returns the parameters or that it at least calls the results of training function\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-55-6fc911b09329>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mRPC_get_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-51-4b15917005f3>\u001B[0m in \u001B[0;36mRPC_get_parameters\u001B[0;34m(data, model, parameters, weights)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;31m# Gather the data sizes on the server\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mtensor_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0mtensor_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor_weights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;31m# Convert all tensors back to weights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data_size' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = model.parameters()\n",
    "weights = []\n",
    "\n",
    "RPC_get_parameters(data, model, parameters, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# averaging of returned parameters \n",
    "\n",
    "def RPC_average_parameters_weighted(data, model, parameters, weights):\n",
    "    \"\"\"\n",
    "    Get parameters from nodes and calculate the average\n",
    "    :param model: torch model\n",
    "    :param parameters: parameters of model\n",
    "    :param weights:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = RPC_get_parameters() # makes returned parameters from RPC_get_parameters the parameters used in this function\n",
    "\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for parameters in model.parameters():\n",
    "            average = sum(x * y for x, y in zip(parameters[i], weights)) / sum(weights)\n",
    "            parameters.data = average\n",
    "            i = i + 1\n",
    "        return {\n",
    "            \"params_weighted\": parameters,\n",
    "        }\n",
    "    \n",
    "\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#     # The first entry of the provided parameters when using dist.gather\n",
    "#     # method also contains the value from the server, remove that one\n",
    "#     minus_server = parameters[i][1:]\n",
    "#     # Calculate the average by summing and dividing by the number of\n",
    "#     # workers\n",
    "#     s = sum(minus_server)\n",
    "#     average = s/len(minus_server)\n",
    "#     # Change the parameter of the global model to the average\n",
    "#     param.data = average\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# training with those averaged parameters\n",
    "\n",
    "def RPC_fed_avg(data, local_dp, model, device, optimizer, epoch, delta=1e-5):\n",
    "    \"\"\"\n",
    "    Training and testing the model on the workers concurrently using federated\n",
    "    averaging, which means calculating the average of the local model\n",
    "    parameters after a number of (local) epochs each training round.\n",
    "\n",
    "    In vantage6, this method will be the training of the model with the average parameters (weighted)\n",
    "\n",
    "    Returns:\n",
    "        Returns the final model\n",
    "    \"\"\"\n",
    "    # TODO: local: since we usually just get the parameters, this well be an entire task, therefore, we might need to train for each individually\n",
    "\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        # Train the model on the workers again\n",
    "        RPC_train(data, local_dp, model, device, optimizer, epoch, delta=1e-5)\n",
    "        # Test the model on the workers\n",
    "        RPC_test(data, model, device)\n",
    "\n",
    "    gather_params = model.get_parameters() # or model.parameters()\n",
    "\n",
    "    RPC_train(model.RPC_average_parameters_weighted(gather_params))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "    ## OR \n",
    "\n",
    "    parameters = RPC_average_parameters_weighted(data, model, parameters, weights) # then uses those parameters for training\n",
    "\n",
    "\n",
    "\n",
    "        # # Gather the parameters after the training round on the server\n",
    "        #     gather_params = coor.gather_parameters(rank, model, group_size + 1, subgroup)\n",
    "        #\n",
    "        #     # If the server\n",
    "        #     if rank == 0:\n",
    "        #         # Calculate the average of the parameters and adjust global model\n",
    "        #         coor.average_parameters_weighted(model, gather_params, weights)\n",
    "        #\n",
    "        #     # Send the new model parameters to the workers\n",
    "        #     coor.broadcast_parameters(model, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTODO, the RPC_initialize_training method is \\ncalled in the RPC_train method, \\nand yet it doesn't know where to get model, device, optimizer from. \\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "These are the parameters needed for the function\n",
    "Data loading and transforming (this will be done beforehand \n",
    "and then stored in './local/training.pt' and './testing.pt')\n",
    "\"\"\"\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "gamma=0.7\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "data = data_loader\n",
    "\n",
    "model = Net()\n",
    "\n",
    "local_dp = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "\"\"\"\n",
    "TODO, the RPC_initialize_training method is \n",
    "called in the RPC_train method, \n",
    "and yet it doesn't know where to get model, device, optimizer from. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3027398176193237\n"
     ]
    }
   ],
   "source": [
    "local_dp = False\n",
    "# RPC_initialize_training(data, gamma, learning_rate, local_dp)\n",
    "\n",
    "# RPC_train(data, local_dp, model, device, optimizer, epoch)\n",
    "\n",
    "# RPC_test(data, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}